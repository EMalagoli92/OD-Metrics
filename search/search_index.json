{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"OD-Metrics","text":"<p>      A python library for Object Detection metrics.    </p>"},{"location":"#why-od-metrics","title":"Why OD-Metrics?","text":"<ul> <li>User-friendly: Designed for simplicity, allowing users to calculate metrics with minimal setup.</li> <li>Highly Customizable: Offers flexibility by allowing users to set custom values for every parameter in metrics definitions.</li> <li>COCOAPI Compatibility: Metrics are rigorously tested to ensure compatibility with COCOAPI, ensuring reliability and consistency.</li> </ul>"},{"location":"#metrics-overview","title":"Metrics Overview","text":"<p>Supported metrics include:</p> <ul> <li>mAP (Mean Average Precision)</li> <li>mAR (Mean Average Recall)</li> <li>IoU (Intersection over Union)</li> </ul> <p>For more information see Metrics.</p>"},{"location":"#installation","title":"Installation","text":"<p>Install from PyPI <pre><code>pip install od-metrics\n</code></pre> Install from Github <pre><code>pip install git+https://github.com/EMalagoli92/OD-Metrics\n</code></pre></p>"},{"location":"#license","title":"License","text":"<p>This work is made available under the Apache License 2.0</p>"},{"location":"#support","title":"Support","text":"<p>Found this helpful? \u2b50 it on GitHub</p>"},{"location":"api_reference/","title":"API Reference","text":"<p>ODMetrics main class implementation.</p>"},{"location":"api_reference/#src.od_metrics.od_metrics.ODMetrics","title":"<code>ODMetrics</code>","text":"<p>ODMetrics class.</p> <p>Compute the Mean-Average-Precision (mAP) and Mean-Average-Recall (mAR) for Object Detection.</p> Source code in <code>src/od_metrics/od_metrics.py</code> <pre><code>class ODMetrics:\n    \"\"\"\n    ODMetrics class.\n\n    Compute the Mean-Average-Precision (mAP) and Mean-Average-Recall (mAR)\n    for Object Detection.\n    \"\"\"\n\n    def __init__(\n            self,\n            iou_thresholds: (float | list[float] | np.ndarray |\n                             type[_Missing]) = _Missing,\n            recall_thresholds: (float | list[float] | np.ndarray |\n                                type[_Missing]) = _Missing,\n            max_detection_thresholds: (int | list[int] | np.ndarray | None |\n                                       type[_Missing]) = _Missing,\n            area_ranges: (dict[str, list[float] | np.ndarray] | None |\n                          type[_Missing]) = _Missing,\n            class_metrics: bool | type[_Missing] = _Missing,\n            box_format: Literal[\"xyxy\", \"xywh\", \"cxcywh\"] = \"xywh\",\n            ) -&gt; None:\n        \"\"\"\n        Initialize.\n\n        Parameters\n        ----------\n        iou_thresholds : float | list[float] | np.ndarray \\\n                          | type[_Missing], optional\n            IoU thresholds.\n            If not specified (`_Missing`), the default (COCO) is used and\n            corresponds to the stepped range `[0.5,...,0.95]` with step\n            `0.05` (`10` values).\n            The default is `_Missing`.\n        recall_thresholds : float | list[float] | np.ndarray \\\n                             | type[_Missing], optional\n            Recall thresholds.\n            If not specified (`_Missing`), the default (COCO) is used and\n            corresponds to the stepped range `[0,...,1]` with step\n            `0.01` (`101` values).\n            The default is `_Missing`.\n        max_detection_thresholds : int | list[int] | np.ndarray | None \\\n                                    | type[_Missing], optional\n            Thresholds on maximum detections per image.\n            If not specified (`_Missing`), the default (COCO) is used and\n            corresponds to the list `[1, 10, 100]`.\n            If `None`, no limit to detections per image will be set.\n            The default is `_Missing`.\n        area_ranges : dict[str, list[float] | np.ndarray] \\\n                       | None | type[_Missing], optional\n            Area ranges.\n            If not specified, the default (COCO) is used and corresponds to:\n            &lt;br&gt;\n            `{\n                \"all\": [0 ** 2, 1e5 ** 2],\n                \"small\": [0 ** 2, 32 ** 2],\n                \"medium\": [32 ** 2, 96 ** 2],\n                \"large\": [96 ** 2, 1e5 ** 2]\n                }`\n            If `None`, no area range limits will be set.\n            The default is `_Missing`.\n        class_metrics : bool | type[_Missing], optional\n            If `True`, evaluation is performed per class:\n            detections are matched to ground truths only if they share\n            the same `label_id`.\n            If `False`, evaluation is category-agnostic.\n            When `True`, the output includes a `\"class_metrics\"`\n            dictionary with per-class results.\n            This corresponds to `useCats` in the COCO evaluation protocol.\n            If not specified (`_Missing`), the default (COCO) is used and\n            corresponds to `True`.\n            The default is `_Missing`.\n        box_format : Literal[\"xyxy\", \"xywh\", \"cxcywh\"], optional\n            Bounding box format.\n            Supported formats are:&lt;br&gt;\n                - `\"xyxy\"`: boxes are represented via corners,\n                        x1, y1 being top left and x2, y2\n                        being bottom right.&lt;br&gt;\n                - `\"xywh\"`: boxes are represented via corner,\n                        width and height, x1, y2 being top\n                        left, w, h being width and height.\n                        This is the default format; all\n                        input formats will be converted\n                        to this.&lt;br&gt;\n                - `\"cxcywh\"`: boxes are represented via centre,\n                        width and height, cx, cy being\n                        center of box, w, h being width\n                        and height.&lt;br&gt;\n            The default is `\"xywh\"`.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        constructor_model = ConstructorModel.model_validate(\n            {\n                \"iou_thresholds\": iou_thresholds,\n                \"recall_thresholds\": recall_thresholds,\n                \"max_detection_thresholds\": max_detection_thresholds,\n                \"area_ranges\": area_ranges,\n                \"class_metrics\": class_metrics,\n                \"box_format\": box_format,\n                },\n            context={\"default_value\": DEFAULT_COCO, \"default_flag\": _Missing}\n            )\n        self.iou_thresholds: np.ndarray = constructor_model.iou_thresholds\n        self.recall_thresholds: np.ndarray = (\n            constructor_model.recall_thresholds)\n        self.max_detection_thresholds: np.ndarray = (\n            constructor_model.max_detection_thresholds)\n        self.area_ranges: dict[str | None, list[float]] = (\n            constructor_model.area_ranges)\n        self.class_metrics: bool = constructor_model.class_metrics\n        self.box_format: Literal[\"xyxy\", \"xywh\", \"cxcywh\"] = (\n            constructor_model.box_format)\n\n    def compute(\n            self,\n            y_true: list[dict],\n            y_pred: list[dict],\n            extended_summary: bool = False,\n            ) -&gt; dict[str, float | int | list[int]\n                      | dict | np.ndarray | partial]:\n        \"\"\"\n        Compute metrics.\n\n        Parameters\n        ----------\n        y_true : list[dict]\n            A list consisting of dictionaries each containing\n            the key-values: each dictionary corresponds to the ground truth\n            of a single image.\n            Parameters that should be provided per dict:\n\n                boxes : list[list[float]] | np.ndarray\n                    List of floats lists or `np.ndarray`; the length of the\n                    list/array correspond to the number of boxes and each\n                    list/array is 4-float specifying the box coordinates in\n                    the format specified in the constructor.\n                labels : list[int] | np.ndarray\n                    List of integers or `np.ndarray` specifying the ground\n                    truth classes for the boxes: the length corresponds to\n                    the number of boxes.\n                iscrowd : list[bool | Literal[0, 1]] | np.ndarray\n                    List of integers or `np.ndarray` specifying crowd regions:\n                    the length corresponds to the number of boxes.\n                    The values can be `bool` or `0`/`1` indicating whether the\n                    bounding box indicates a crowd of objects.\n                    Value is optional, and if not provided it will\n                    automatically be set to `False`.\n                area : list[float] | np.ndarray\n                    A list of `float` or `np.ndarray` specifying the area of\n                    the objects: the length corresponds to the number of\n                    boxes.\n                    Value is optional, and if not provided will\n                    be automatically calculated based on\n                    the bounding box provided.\n                    Only affects which samples contribute the specific\n                    area range.\n        y_pred : list[dict]\n            A list consisting of dictionaries each containing\n            the key-values: each dictionary corresponds to the predictions\n            of a single image.\n            Parameters that should be provided per dict:\n\n                boxes : list[list[float]] | np.ndarray\n                    List of float lists or `np.ndarray`; the length of the\n                    list/array correspond to the number of boxes and each\n                    list/array is 4-float specifying the box coordinates in\n                    the format specified in the constructor.\n                scores : list[float] | np.ndarray\n                    List of floats or `np.ndarray` specifying the score for\n                    the boxes: the length corresponds to the number of boxes.\n                labels : list[int] | np.ndarray\n                    List of integers or `np.ndarray` specifying the ground\n                    truth classes for the boxes: the length corresponds to\n                    the number of boxes.\n        extended_summary : bool, optional\n            Option to enable extended summary with additional metrics\n            including `IoU`, `AP` (Average Precision), `AR` (Average Recall)\n            and `mean_evaluator` (`Callable`).\n            The output dictionary will contain the following extra key-values:\n\n                IoU : dict[tuple[int, int], np.ndarray]\n                       A dictionary containing the IoU values for every\n                       image/class combination e.g. `IoU[(0,0)]`\n                       would contain the IoU for image `0` and class `0`.\n                       Each value is a `np.ndarray` with shape `(n, m)`\n                       where `n` is the number of detections and `m` is\n                       the number of ground truth boxes for that image/class\n                       combination.\n                AP : np.ndarray\n                      Average precision: a `np.ndarray` of shape\n                      `(T, R, K, A, M)` containing the precision values.\n                      Here:\n                          - `T` is the number of IoU thresholds\n                          - `R` is the number of recall thresholds\n                          - `K` is the number of classes\n                          - `A` is the number of areas\n                          - `M` is the number of max detections per image\n                AR : np.ndarray\n                     A `np.ndarray` of shape `(T, K, A, M)` containing the\n                     averag recall values.\n                     Here:\n                         - `T` is the number of IoU thresholds\n                         - `K` is the number of classes\n                         - `A` is the number of areas\n                         - `M` is the number of max detections per image\n                mean_evaluator : Callable\n                    Mean evaluator function.\n                    Parameters are:\n                        iou_threshold : (float | list[float] | np.ndarray\n                                         | None), optional\n                            IoU threshold on which calculate the mean.\n                            It can be a `float`, a list of floats, `np.ndarray`\n                            or `None`; all values must be inlcuded in the\n                            constructor argument `iou_thresholds`.\n                            If `None`, all input `iou_thresholds` will be used.\n                            The default is `None`.\n                        area_range_key : (str | list[str] | np.ndarray\n                                          | None), optional\n                            Area range key on which calculate the mean.\n                            It can be a `str`, a list of strings, `np.ndarray`,\n                            or `None`; all values must be included in the\n                            constructor argument `area_ranges`.\n                            If `None`, all input `area_ranges` keys will be\n                            used.\n                            The default is `None`.\n                        max_detection_threshold : (int | list[int] |\n                                                   np.ndarray | None), optional\n                            Threshold on maxiumum detections per image on\n                            which calculate the mean.\n                            It can be a `int`, a list of integers, `np.ndarray`\n                            or `None`; all values must be inlcuded in the\n                            constructor argument `max_detection_thresholds`.\n                            If `None`, all input `max_detection_thresholds`\n                            will be used.\n                            The default is `None`.\n                        label_id : (int | list[int] | np.ndarray\n                                    | None), optional\n                            Label ids on which calculate the mean.\n                            If `class_metrics` is `True`, `label_id` must be\n                            included in the label ids of the provided `y_true`.\n                            If `class_metrics` is `False`, `label_id` must be\n                            `-1` (in this case equivalent to `None`).\n                            If `None`, all labels will be used.\n                            The default is `None`.\n                        metrics : (Literal[\"AP\", \"AR\"]\n                                   | list[Literal[\"AP\", \"AR\"]] | None),\n                                   optional\n                            Metrics on which calculate the mean.\n                            If `None`, both `\"AP\"` and `\"AR\"` will be used.\n                            The default is `None`.\n                        include_spec : bool, optional\n                            Whether to include mean settings specification.\n                            The default is `False`.\n                        prefix : str, optional\n                            Prefix to add to metrics keys.\n                            The default is `m`.\n\n        Returns\n        -------\n        dict[str, float | int | list[int] | dict | np.ndarray | partial]\n            The format of the output string metric ids is defined as:\n\n            `{metric}@[{iou_thresholds} | {area_ranges}\n                      | {max_detection_thresholds}]`\n\n            If a field is `None`, the corresponding string field will be emtpy,\n            e.g., `{metric}@[{iou_thresholds} | {area_ranges}]`\n            indicate metrics calculated without limit to detections per image,\n            i.e. `max_detections_thresholds` set to `None`.\n            Assuming that the parameters passed to the constructor\n            are the default ones (COCO), the output dictionary will\n            contain the following key-values: &lt;br&gt;\n                `mAP@[.5 | all | 100]` &lt;br&gt;\n                `mAR@[.5 | all | 100]` &lt;br&gt;\n                `mAP@[.75 | all | 100]` &lt;br&gt;\n                `mAR@[.75 | all | 100]` &lt;br&gt;\n                `mAR@[.5:.95 | all | 1]` &lt;br&gt;\n                `mAR@[.5:.95 | all | 10]` &lt;br&gt;\n                `mAR@[.5:.95 | all | 100]` &lt;br&gt;\n                `mAP@[.5:.95 | all | 100]` &lt;br&gt;\n                `mAP@[.5:.95 | large | 100]` &lt;br&gt;\n                `mAR@[.5:.95 | large | 100]` &lt;br&gt;\n                `mAP@[.5:.95 | medium | 100]` &lt;br&gt;\n                `mAR@[.5:.95 | medium | 100]` &lt;br&gt;\n                `mAP@[.5:.95 | small | 100]` &lt;br&gt;\n                `mAR@[.5:.95 | small | 100]` &lt;br&gt;\n            If `class_metrics` is `True`, the output dictionary will contain\n            the additional key `class_metrics`, a dictionary with class as key\n            and value each of the above metrics.\n            If `extended_summary` is `True`, the output dictionary will contain\n            the additional keys `IoU`, `AP`, `AR` and `mean_evaluator`.\n            (See `extended_summary`)\n        \"\"\"\n        # Image ids\n        images_ids = list(range(len(y_true)))\n\n        # Parse Annotations\n        y_true, y_pred = ComputeModel.model_validate(\n            y_true=y_true,\n            y_pred=y_pred,\n            extended_summary=extended_summary,\n            box_format=self.box_format\n        )\n\n        # Get label_ids from y_true + y_pred\n        label_ids = np.unique([_annotation[\"label_id\"]\n                               for _annotation in y_true + y_pred]).tolist()\n        _label_ids = label_ids if self.class_metrics else [-1]\n\n        # y_true, y_pred --&gt; default_dict (keys: (image_id, label_id))\n        y_true_ddict = defaultdict(\n            list,\n            [(k, list(v)) for k, v in groupby(\n                sorted(y_true, key=lambda x: (x[\"image_id\"], x[\"label_id\"])),\n                key=lambda x: (x[\"image_id\"], x[\"label_id\"]))]\n            )\n        y_pred_ddict = defaultdict(\n            list,\n            [(k, list(v)) for k, v in groupby(\n                sorted(y_pred, key=lambda x: (x[\"image_id\"], x[\"label_id\"])),\n                key=lambda x: (x[\"image_id\"], x[\"label_id\"]))]\n            )\n\n        # Compute IoU\n        ious = {(image_id, label_id): self._compute_iou(\n            y_true=y_true_ddict,\n            y_pred=y_pred_ddict,\n            image_id=image_id,\n            label_id=label_id,\n            label_ids=label_ids,\n            ) for image_id, label_id in product(images_ids, _label_ids)\n            }\n\n        # Evaluate each image\n        images_results = [self._evaluate_image(\n            y_true=y_true_ddict,\n            y_pred=y_pred_ddict,\n            image_id=image_id,\n            label_id=label_id,\n            label_ids=label_ids,\n            area_range=_area_range,\n            ious=ious\n            ) for label_id, _area_range, image_id in\n            product(_label_ids, self.area_ranges.values(), images_ids)\n        ]\n\n        # Aggregate results\n        results = self._aggregate(\n            label_ids=_label_ids,\n            images_ids=images_ids,\n            images_results=images_results,\n            )\n\n        # Mean evaluator\n        mean_evaluator = partial(\n            self._get_mean,\n            results=results,\n            label_ids=_label_ids\n            )\n\n        # Get standard output values (globally)\n        standard_results = self._get_standard(\n            mean_evaluator=mean_evaluator,\n            label_id=None,\n            prefix=\"m\",\n            )\n\n        # Prepare output\n        output: dict[str, float | int | list[int]\n                     | dict | np.ndarray | partial] = {}\n        output |= standard_results\n\n        # Class metrics\n        if self.class_metrics:\n            output |= {\n                \"class_metrics\": {\n                    label_id: self._get_standard(\n                        mean_evaluator=mean_evaluator,\n                        label_id=label_id,\n                        prefix=\"\",\n                        ) for label_id in _label_ids\n                        }\n                }\n\n        # Add metadata\n        output |= {\n            \"classes\": label_ids,\n            \"n_images\": len(images_ids)\n            }\n\n        if extended_summary:\n            output |= ({k: v for k, v in results.items() if k in [\"AP\", \"AR\"]}\n                       | {\n                           \"IoU\": ious,\n                           \"mean_evaluator\": mean_evaluator,\n                           }\n                       )\n\n        return output\n\n    def _compute_iou(\n            self,\n            y_true: defaultdict,\n            y_pred: defaultdict,\n            image_id: int,\n            label_id: int,\n            label_ids: list[int],\n            ) -&gt; np.ndarray:\n        \"\"\"\n        Compute IoU.\n\n        Parameters\n        ----------\n        y_true : defaultdict\n            Ground truths.\n        y_pred : defaultdict\n            Predictions.\n        image_id : int\n            Image id.\n        label_id : int\n            Label id.\n        label_ids : list[int]\n            Overall label ids.\n\n        Returns\n        -------\n        np.ndarray\n            `np.ndarray` containing IoU values between `y_true` and `y_pred`.\n        \"\"\"\n        if self.class_metrics:\n            y_true_ = y_true[image_id, label_id]\n            y_pred_ = y_pred[image_id, label_id]\n        else:\n            y_true_ = reduce(\n                lambda x, y: x+y,\n                [y_true[image_id, label_id] for label_id in label_ids],\n                [],\n                )\n            y_pred_ = reduce(\n                lambda x, y: x+y,\n                [y_pred[image_id, label_id] for label_id in label_ids],\n                [],\n                )\n\n        if not y_pred_ and not y_true_:\n            return np.array([])\n\n        # Sort predictions highest score first and cut off to\n        # max_detection_thresholds\n        y_pred_ = sorted(\n            y_pred_,\n            key=operator.itemgetter(\"score\"),\n            reverse=True\n            )[: self.max_detection_thresholds[-1]]\n\n        y_true_boxes = [yt[\"bbox\"] for yt in y_true_]\n        y_pred_boxes = [yp[\"bbox\"] for yp in y_pred_]\n        iscrowd = [yt[\"iscrowd\"] for yt in y_true_]\n\n        # Compute IoU between each prediction and ground truth region\n        ious = iou(\n            y_true=y_true_boxes,\n            y_pred=y_pred_boxes,\n            iscrowd=iscrowd,\n            box_format=\"xywh\",\n            )\n        return ious\n\n    def _evaluate_image(  # pylint: disable=R0912\n            self,\n            y_true: defaultdict,\n            y_pred: defaultdict,\n            image_id: int,\n            label_id: int,\n            label_ids: list[int],\n            area_range: list[float],\n            ious: dict,\n            ) -&gt; dict[str, Any] | None:\n        \"\"\"\n        Evaluate metrics for a single image.\n\n        Parameters\n        ----------\n        y_true : defaultdict\n            Ground truths.\n        y_pred : defaultdict\n            Predictions.\n        image_id : int\n            Image id.\n        label_id : int\n            Label id.\n        label_ids : list[int]\n            Overall label ids.\n        area_range : list[float]\n            Area range.\n        ious : dict\n            IoU dictionary.\n\n        Returns\n        -------\n        dict[str, Any] | None\n            Dictionary containing results given image and label.\n            `None` if there is no ground-truths and detections for that\n            specific `image_id` and `label_id`.\n        \"\"\"\n        if self.class_metrics:\n            y_true_ = y_true[image_id, label_id]\n            y_pred_ = y_pred[image_id, label_id]\n        else:\n            y_true_ = reduce(\n                lambda x, y: x+y,\n                [y_true[image_id, label_id] for label_id in label_ids],\n                [],\n                )\n            y_pred_ = reduce(\n                lambda x, y: x+y,\n                [y_pred[image_id, label_id] for label_id in label_ids],\n                [],\n                )\n        if not y_true_ and not y_pred_:\n            return None\n\n        # Assign _ignore if ignore or outside area range.\n        for yt_ in y_true_:\n            if (\n                    yt_[\"ignore\"] or (yt_[\"area\"] &lt; area_range[0]\n                                      or yt_[\"area\"] &gt; area_range[1])\n            ):\n                yt_[\"_ignore\"] = 1\n            else:\n                yt_[\"_ignore\"] = 0\n\n        # Sort y_true ignore last\n        if len(y_true_) == 0:\n            y_true_indexes = ()\n        else:\n            y_true_indexes, y_true_ = zip(*sorted(\n                enumerate(y_true_), key=lambda x: x[1][\"_ignore\"]))\n        iscrowd = [int(yt[\"iscrowd\"]) for yt in y_true_]\n\n        # Sort y_pred highest score first and cut off y_pred to\n        # max detection threshold.\n        y_pred_ = sorted(y_pred_, key=operator.itemgetter(\"score\"),\n                         reverse=True)[: self.max_detection_thresholds[-1]]\n\n        # Load computed ious\n        ious = (ious[image_id, label_id][:, y_true_indexes]\n                if len(ious[image_id, label_id]) &gt; 0\n                else ious[image_id, label_id]\n                )\n\n        iou_thresholds_len = len(self.iou_thresholds)\n        y_true_len = len(y_true_)\n        y_pred_len = len(y_pred_)\n        # y_true_matches and y_pred_matches will contain ids of\n        # matched prediction and ground truths respectively.\n        y_true_matches = np.zeros((iou_thresholds_len, y_true_len))\n        y_pred_matches = np.zeros((iou_thresholds_len, y_pred_len))\n        y_true_ignore = np.array([yt[\"_ignore\"] for yt in y_true_])\n        y_pred_ignore = np.zeros((iou_thresholds_len, y_pred_len))\n        if not len(ious) == 0:\n            for iou_threshold_index, iou_threshold in enumerate(\n                    self.iou_thresholds):\n                for yp_index, yp_ in enumerate(y_pred_):\n                    iou_ = min([iou_threshold, 1-1e-10])\n                    # Information about best match so far\n                    # match=-1 -&gt; Unmatched\n                    match_ = -1\n                    for yt_index, yt_ in enumerate(y_true_):\n                        # If this yt already matched, and not a crowd, continue\n                        if (y_true_matches[iou_threshold_index, yt_index] &gt; 0\n                                and not iscrowd[yt_index]):\n                            continue\n                        # If yp matched to a previous gt (not ignore) and\n                        # the new is ingore, stop\n                        if (match_ &gt; -1 and y_true_ignore[match_] == 0\n                                and y_true_ignore[yt_index] == 1):\n                            break\n                        # If iou between yp and yt &lt; iou threshold, continue\n                        if ious[yp_index, yt_index] &lt; iou_:\n                            continue\n                        # If match successful and best so far,\n                        # store appropriately\n                        iou_ = ious[yp_index, yt_index]\n                        match_ = yt_index\n                    if match_ == -1:\n                        continue\n                    # If match made store id of match for both dt and gt\n                    y_pred_ignore[iou_threshold_index, yp_index] = (\n                        y_true_ignore[match_])\n                    y_pred_matches[iou_threshold_index, yp_index] = (\n                        y_true_[match_][\"id\"])\n                    y_true_matches[iou_threshold_index, match_] = yp_[\"id\"]\n        # set unmatched detections outside of area range to ignore\n        out_area = np.array(\n            [yp[\"area\"] &lt; area_range[0] or yp[\"area\"] &gt; area_range[1]\n             for yp in y_pred_]).reshape((1, len(y_pred_)))\n        y_pred_ignore = np.logical_or(\n            y_pred_ignore,\n            np.logical_and(y_pred_matches == 0,\n                           np.repeat(out_area, iou_thresholds_len, 0))\n            )\n        # store results for given image and label\n        return {\n                \"image_id\": image_id,\n                \"label_id\": label_id,\n                \"area_range\": area_range,\n                \"max_detection_threshold\": self.max_detection_thresholds[-1],\n                \"y_pred_indexes\": [yp[\"id\"] for yp in y_pred_],\n                \"y_true_indexes\": [yt[\"id\"] for yt in y_true_],\n                \"y_pred_matches\": y_pred_matches,\n                \"y_true_matches\": y_true_matches,\n                \"y_pred_scores\": [yp[\"score\"] for yp in y_pred_],\n                \"y_true_ignore\": y_true_ignore,\n                \"y_pred_ignore\": y_pred_ignore,\n                }\n\n    def _aggregate(  # pylint: disable=R0915\n            self,\n            label_ids: list[int],\n            images_ids: list[int],\n            images_results: list[dict[str, Any] | None]\n            ) -&gt; dict[str, np.ndarray]:\n        \"\"\"\n        Aggregate images results.\n\n        Parameters\n        ----------\n        label_ids : list[int]\n            Overall label ids.\n        images_ids : list[int]\n            Overall image ids.\n        images_results : list[dict[str, Any] | None]\n            List of dictionaries containing images results.\n\n        Returns\n        -------\n        dict[str, np.ndarray]\n            Aggregated results.\n        \"\"\"\n        # Settings\n        iou_thresholds_len = len(self.iou_thresholds)\n        recall_thresholds_len = len(self.recall_thresholds)\n        label_ids_len = len(label_ids)\n        area_range_len = len(self.area_ranges)\n        max_detection_thresholds_len = len(self.max_detection_thresholds)\n        images_ids_len = len(images_ids)\n\n        # Initialize\n        average_precision = -np.ones((\n            iou_thresholds_len,\n            recall_thresholds_len,\n            label_ids_len,\n            area_range_len,\n            max_detection_thresholds_len\n            )\n        )\n        average_recall = -np.ones((\n            iou_thresholds_len,\n            label_ids_len,\n            area_range_len,\n            max_detection_thresholds_len\n            )\n        )\n        scores = -np.ones((\n            iou_thresholds_len,\n            recall_thresholds_len,\n            label_ids_len,\n            area_range_len,\n            max_detection_thresholds_len\n            )\n        )\n\n        # Retrieve images_results at each label, area range,\n        # and max number of detections\n        for label_id_index, _ in enumerate(label_ids):  # pylint: disable=R1702\n            n_label = label_id_index*area_range_len*images_ids_len\n            for area_range_index, _ in enumerate(self.area_ranges):\n                n_area_range = area_range_index*images_ids_len\n                for max_det_index, max_det in enumerate(\n                        self.max_detection_thresholds):\n                    images_results_full = [\n                        images_results[n_label + n_area_range + i]\n                        for i, _ in enumerate(images_ids)\n                        ]\n                    images_results_ = [e for e in images_results_full\n                                       if e is not None]\n                    if len(images_results_) == 0:\n                        continue\n                    y_pred_scores = np.concatenate(\n                        [e[\"y_pred_scores\"][:max_det]\n                         for e in images_results_]\n                        )\n\n                    if len(y_pred_scores) == 0:\n                        indexes = ()\n                        y_pred_scores_sorted = y_pred_scores\n                    else:\n                        indexes, y_pred_scores_sorted = zip(*sorted(\n                            enumerate(y_pred_scores),\n                            key=lambda x: x[1],\n                            reverse=True)\n                            )\n\n                    y_pred_matches = np.concatenate(\n                        [e[\"y_pred_matches\"][:, :max_det]\n                         for e in images_results_],\n                        axis=1\n                        )[:, indexes]\n                    y_pred_ignore = np.concatenate(\n                        [e[\"y_pred_ignore\"][:, :max_det]\n                         for e in images_results_],\n                        axis=1\n                        )[:, indexes]\n                    y_true_ignore = np.concatenate(\n                        [e[\"y_true_ignore\"] for e in images_results_])\n                    not_ignore = np.count_nonzero(y_true_ignore == 0)\n                    if not_ignore == 0:\n                        continue\n                    tps = np.logical_and(\n                        y_pred_matches,\n                        np.logical_not(y_pred_ignore)\n                        )\n                    fps = np.logical_and(\n                        np.logical_not(y_pred_matches),\n                        np.logical_not(y_pred_ignore)\n                        )\n\n                    tp_sum = np.cumsum(tps, axis=1)\\\n                        .astype(dtype=float)\n                    fp_sum = np.cumsum(fps, axis=1)\\\n                        .astype(dtype=float)\n                    for iou_threshold_index, (tp_, fp_) in enumerate(\n                            zip(tp_sum, fp_sum)):\n                        tp_ = np.array(tp_)\n                        fp_ = np.array(fp_)\n                        n_d = len(tp_)\n                        recall = tp_ / not_ignore\n                        precision = tp_ / (fp_+tp_+np.spacing(1))\n                        qty = np.zeros((recall_thresholds_len,))\n                        ss_ = np.zeros((recall_thresholds_len,))\n\n                        if n_d:\n                            average_recall[\n                                iou_threshold_index,\n                                label_id_index,\n                                area_range_index,\n                                max_det_index\n                                ] = recall[-1]\n                        else:\n                            average_recall[\n                                iou_threshold_index,\n                                label_id_index,\n                                area_range_index,\n                                max_det_index\n                                ] = 0\n\n                        # Interpolation P-R Curve\n                        interpolated_precision = np.maximum.accumulate(\n                            precision[::-1])[::-1]\n\n                        indexes_ = np.searchsorted(\n                            recall,\n                            self.recall_thresholds,\n                            side=\"left\"\n                            )\n                        try:\n                            for ri_, pi_ in enumerate(indexes_):\n                                qty[ri_] = interpolated_precision[pi_]\n                                ss_[ri_] = y_pred_scores_sorted[pi_]\n                        except IndexError:\n                            pass\n                        average_precision[\n                            iou_threshold_index,\n                            :,\n                            label_id_index,\n                            area_range_index,\n                            max_det_index\n                            ] = np.array(qty)\n                        scores[\n                            iou_threshold_index,\n                            :,\n                            label_id_index,\n                            area_range_index,\n                            max_det_index\n                            ] = np.array(ss_)\n        return {\n            \"AP\": average_precision,\n            \"AR\": average_recall\n            }\n\n    def _get_mean(\n            self,\n            iou_threshold: float | list[float] | np.ndarray | None = None,\n            area_range_key: str | list[str] | np.ndarray | None = None,\n            max_detection_threshold: (int | list[int] | np.ndarray\n                                      | None) = None,\n            label_id: int | list[int] | np.ndarray | None = None,\n            metrics: (Literal[\"AP\", \"AR\"] | list[Literal[\"AP\", \"AR\"]]\n                      | None) = None,\n            include_spec: bool = False,\n            prefix: str = \"m\",\n            results: dict[str, Any] | type[_Missing] = _Missing,\n            label_ids: list[int] | np.ndarray | type[_Missing] = _Missing,\n            ) -&gt; dict[str, float | dict[str, list]]:\n        \"\"\"\n        Calculate mean for Average-Precision (mAP) and Average-Recall (mAR).\n\n        Parameters\n        ----------\n        iou_threshold : float | list[float] | np.ndarray | None, optional\n            IoU threshold on which calculate the mean.\n            It can be a `float`, a list of floats, `np.ndarray`\n            or `None`; all values must be inlcuded in the\n            constructor argument `iou_thresholds`.\n            If `None`, all input `iou_thresholds` will be used.\n            The default is `None`.\n        area_range_key : str | list[str] | np.ndarray | None, optional\n            Area range key on which calculate the mean.\n            It can be a `str`, a list of strings, `np.ndarray`,\n            or `None`; all values must be included in the\n            constructor argument `area_ranges`.\n            If `None`, all input `area_ranges` keys will be used.\n            The default is `None`.\n        max_detection_threshold : int | list[int] |\n                                   np.ndarray | None), optional\n            Threshold on maximum detections per image on\n            which calculate the mean.\n            It can be a `int`, a list of integers, `np.ndarray`\n            or `None`; all values must be inlcuded in the\n            constructor argument `max_detection_thresholds`.\n            If `None`, all input `max_detection_thresholds`\n            will be used.\n            The default is `None`.\n        label_id : int | list[int] | np.ndarray | None, optional\n            Label ids on which calculate the mean.\n            If `class_metrics` is `True`, `label_id` must be\n            included in the label ids of the provided `y_true`.\n            If `class_metrics` is `False`, `label_id` must be `-1`\n            (in this case equivalent to `None`).\n            If `None`, all labels will be used.\n            The default is `None`.\n        metrics : Literal[\"AP\", \"AR\"] | list[Literal[\"AP\", \"AR\"]] | None,\n                  optional\n            Metrics on which calculate the mean.\n            If `None`, both `\"AP\"` and `\"AR\"` will be used.\n            The default is `None`.\n        include_spec : bool, optional\n            Whether to include mean settings specification.\n            The default is `False`.\n        prefix : str, optional\n            Prefix to add to metrics keys.\n            The default is `m`.\n        results : dict[str, Any] | type[_Missing], optional\n            Dictionary containing aggregated images results.\n            If `_Missing` an error will be raised.\n            The default is `_Missing`.\n        label_ids : list[int] | np.ndarray | type[_Missing], optional\n            All label ids found in `y_true`.\n            If `_Missing` an error will be raised.\n            The default is `_Missing`.\n\n        Returns\n        -------\n        dict[str, float | dict[str, list]]\n            Dictionary containing the values of precision and recall.\n            If `include_spec` input parameters info will be added.\n        \"\"\"\n        # Sanity check\n        # results\n        if results is _Missing:\n            raise TypeError(\"`results` must be provided.\")\n        results_ = cast(dict[str, Any], results)\n        # label_ids\n        if label_ids is _Missing:\n            raise TypeError(\"`label_ids` must be passed.\")\n\n        # Default\n        default_value = {\n            \"iou_threshold\": self.iou_thresholds,\n            \"label_id\": np.array(label_ids),\n            \"area_range_key\": np.array(list(self.area_ranges.keys())),\n            \"max_detection_threshold\": self.max_detection_thresholds,\n            }\n\n        mean_model = MeanModel.model_validate(\n            {\n                \"iou_threshold\": iou_threshold,\n                \"area_range_key\": area_range_key,\n                \"max_detection_threshold\": max_detection_threshold,\n                \"label_id\": label_id,\n                \"metrics\": metrics,\n                \"include_spec\": include_spec,\n                \"prefix\": prefix\n                },\n            context={\"default_value\": default_value, \"default_flag\": None}\n            )\n        metrics = mean_model.metrics\n        include_spec = mean_model.include_spec\n        prefix = mean_model.prefix\n\n        mean_params = {\n            \"iou_threshold\": mean_model.iou_threshold,\n            \"label_id\": mean_model.label_id,\n            \"area_range_key\": mean_model.area_range_key,\n            \"max_detection_threshold\": mean_model.max_detection_threshold,\n            }\n\n        # Indexes\n        indexes = {\n            key: get_indexes(default_value[key], value)\n            for key, value in mean_params.items()\n            }\n\n        # Sanity check\n        for key, value in mean_params.items():\n            if len(value) != len(indexes[key]):\n                raise ValueError(\n                    f\"Input parameter {key}: {value} not found \"\n                    f\"in initial settings which includes {default_value[key]}.\"\n                    )\n\n        # Assigns slice(None) if the indices match the entire dimension\n        # of results array.\n        indexes_ = {\n            key: (\n                np.array([slice(None)]) if np.equal(\n                    indexes[key],\n                    np.arange(len(default_value[key]))\n                    ).all() else indexes[key])\n            for key in indexes.keys()\n        }\n\n        combinations_indexes = [\n            dict(zip(indexes_.keys(), values))\n            for values in product(*(indexes_[key] for key in indexes_.keys()))\n            ]\n\n        output: dict[str, float | dict[str, list]] = {}\n        suffix = get_suffix(\n            iou_threshold=mean_params[\"iou_threshold\"],\n            area_range_key=mean_params[\"area_range_key\"],\n            max_detection_threshold=mean_params[\"max_detection_threshold\"]\n            )\n        for metric in metrics:\n            slices: list[tuple]\n            if metric == \"AP\":\n                slices = [(\n                    index[\"iou_threshold\"],\n                    slice(None),\n                    index[\"label_id\"],\n                    index[\"area_range_key\"],\n                    index[\"max_detection_threshold\"],\n                    ) for index in combinations_indexes]\n            elif metric == \"AR\":\n                slices = [(\n                    index[\"iou_threshold\"],\n                    index[\"label_id\"],\n                    index[\"area_range_key\"],\n                    index[\"max_detection_threshold\"],\n                    ) for index in combinations_indexes]\n            values = np.stack([results_[metric][slice_] for slice_ in slices])\n            values = values[values &gt; -1]\n            mean_values = -1 if len(values) == 0 else np.mean(values)\n            output[f\"{prefix}{metric}{suffix}\"] = float(mean_values)\n\n        if include_spec:\n            output |= {\n                \"spec\": {key: value.tolist()\n                         for key, value in mean_params.items()}\n                }\n\n        return output\n\n    def _get_standard(\n            self,\n            mean_evaluator: Callable,\n            label_id: int | list[int] | np.ndarray | None,\n            prefix: str,\n            ) -&gt; dict[str, float | dict[str, list]]:\n        \"\"\"\n        Get standard metrics output.\n\n        Parameters\n        ----------\n        mean_evaluator : Callable\n            Mean evaluator function.\n        label_id : int | list[int] | np.ndarray | None\n            Label ids.\n            If `None`, all labels will be used.\n            The default is `None`.\n        prefix : str\n            Prefix to add to metrics keys.\n\n        Returns\n        -------\n        dict[str, float | dict[str, list]]\n            Dictionary with metrics output.\n        \"\"\"\n        standard_params = []\n\n        for combination in copy.deepcopy(_STANDARD_OUTPUT):\n            if (combination[\"iou_threshold\"] is not None) and (\n                    combination[\"iou_threshold\"] not in self.iou_thresholds):\n                combination[\"iou_threshold\"] = None\n\n            if combination[\"area_range_key\"] not in self.area_ranges.keys():\n                combination[\"area_range_key\"] = None\n\n            if (combination[\"max_detection_threshold\"] not in\n                    self.max_detection_thresholds):\n                combination[\"max_detection_threshold\"] = None\n            standard_params.append(combination)\n\n        output: dict[str, float | dict[str, list]] = {}\n        for _param in standard_params:\n            output |= mean_evaluator(\n                **_param,\n                include_spec=False,\n                label_id=label_id,\n                prefix=prefix,\n                )\n\n        output = dict(sorted(output.items()))\n        return output\n</code></pre>"},{"location":"api_reference/#src.od_metrics.od_metrics.ODMetrics.__init__","title":"<code>__init__(iou_thresholds=_Missing, recall_thresholds=_Missing, max_detection_thresholds=_Missing, area_ranges=_Missing, class_metrics=_Missing, box_format='xywh')</code>","text":"<p>Initialize.</p> <p>Parameters:</p> Name Type Description Default <code>iou_thresholds</code> <code>float | list[float] | ndarray | type[_Missing]</code> <p>IoU thresholds. If not specified (<code>_Missing</code>), the default (COCO) is used and corresponds to the stepped range <code>[0.5,...,0.95]</code> with step <code>0.05</code> (<code>10</code> values). The default is <code>_Missing</code>.</p> <code>_Missing</code> <code>recall_thresholds</code> <code>float | list[float] | ndarray | type[_Missing]</code> <p>Recall thresholds. If not specified (<code>_Missing</code>), the default (COCO) is used and corresponds to the stepped range <code>[0,...,1]</code> with step <code>0.01</code> (<code>101</code> values). The default is <code>_Missing</code>.</p> <code>_Missing</code> <code>max_detection_thresholds</code> <code>int | list[int] | ndarray | None | type[_Missing]</code> <p>Thresholds on maximum detections per image. If not specified (<code>_Missing</code>), the default (COCO) is used and corresponds to the list <code>[1, 10, 100]</code>. If <code>None</code>, no limit to detections per image will be set. The default is <code>_Missing</code>.</p> <code>_Missing</code> <code>area_ranges</code> <code>dict[str, list[float] | ndarray] | None | type[_Missing]</code> <p>Area ranges. If not specified, the default (COCO) is used and corresponds to:  <code>{     \"all\": [0 ** 2, 1e5 ** 2],     \"small\": [0 ** 2, 32 ** 2],     \"medium\": [32 ** 2, 96 ** 2],     \"large\": [96 ** 2, 1e5 ** 2]     }</code> If <code>None</code>, no area range limits will be set. The default is <code>_Missing</code>.</p> <code>_Missing</code> <code>class_metrics</code> <code>bool | type[_Missing]</code> <p>If <code>True</code>, evaluation is performed per class: detections are matched to ground truths only if they share the same <code>label_id</code>. If <code>False</code>, evaluation is category-agnostic. When <code>True</code>, the output includes a <code>\"class_metrics\"</code> dictionary with per-class results. This corresponds to <code>useCats</code> in the COCO evaluation protocol. If not specified (<code>_Missing</code>), the default (COCO) is used and corresponds to <code>True</code>. The default is <code>_Missing</code>.</p> <code>_Missing</code> <code>box_format</code> <code>Literal['xyxy', 'xywh', 'cxcywh']</code> <p>Bounding box format. Supported formats are:     - <code>\"xyxy\"</code>: boxes are represented via corners,             x1, y1 being top left and x2, y2             being bottom right.     - <code>\"xywh\"</code>: boxes are represented via corner,             width and height, x1, y2 being top             left, w, h being width and height.             This is the default format; all             input formats will be converted             to this.     - <code>\"cxcywh\"</code>: boxes are represented via centre,             width and height, cx, cy being             center of box, w, h being width             and height. The default is <code>\"xywh\"</code>.</p> <code>'xywh'</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/od_metrics/od_metrics.py</code> <pre><code>def __init__(\n        self,\n        iou_thresholds: (float | list[float] | np.ndarray |\n                         type[_Missing]) = _Missing,\n        recall_thresholds: (float | list[float] | np.ndarray |\n                            type[_Missing]) = _Missing,\n        max_detection_thresholds: (int | list[int] | np.ndarray | None |\n                                   type[_Missing]) = _Missing,\n        area_ranges: (dict[str, list[float] | np.ndarray] | None |\n                      type[_Missing]) = _Missing,\n        class_metrics: bool | type[_Missing] = _Missing,\n        box_format: Literal[\"xyxy\", \"xywh\", \"cxcywh\"] = \"xywh\",\n        ) -&gt; None:\n    \"\"\"\n    Initialize.\n\n    Parameters\n    ----------\n    iou_thresholds : float | list[float] | np.ndarray \\\n                      | type[_Missing], optional\n        IoU thresholds.\n        If not specified (`_Missing`), the default (COCO) is used and\n        corresponds to the stepped range `[0.5,...,0.95]` with step\n        `0.05` (`10` values).\n        The default is `_Missing`.\n    recall_thresholds : float | list[float] | np.ndarray \\\n                         | type[_Missing], optional\n        Recall thresholds.\n        If not specified (`_Missing`), the default (COCO) is used and\n        corresponds to the stepped range `[0,...,1]` with step\n        `0.01` (`101` values).\n        The default is `_Missing`.\n    max_detection_thresholds : int | list[int] | np.ndarray | None \\\n                                | type[_Missing], optional\n        Thresholds on maximum detections per image.\n        If not specified (`_Missing`), the default (COCO) is used and\n        corresponds to the list `[1, 10, 100]`.\n        If `None`, no limit to detections per image will be set.\n        The default is `_Missing`.\n    area_ranges : dict[str, list[float] | np.ndarray] \\\n                   | None | type[_Missing], optional\n        Area ranges.\n        If not specified, the default (COCO) is used and corresponds to:\n        &lt;br&gt;\n        `{\n            \"all\": [0 ** 2, 1e5 ** 2],\n            \"small\": [0 ** 2, 32 ** 2],\n            \"medium\": [32 ** 2, 96 ** 2],\n            \"large\": [96 ** 2, 1e5 ** 2]\n            }`\n        If `None`, no area range limits will be set.\n        The default is `_Missing`.\n    class_metrics : bool | type[_Missing], optional\n        If `True`, evaluation is performed per class:\n        detections are matched to ground truths only if they share\n        the same `label_id`.\n        If `False`, evaluation is category-agnostic.\n        When `True`, the output includes a `\"class_metrics\"`\n        dictionary with per-class results.\n        This corresponds to `useCats` in the COCO evaluation protocol.\n        If not specified (`_Missing`), the default (COCO) is used and\n        corresponds to `True`.\n        The default is `_Missing`.\n    box_format : Literal[\"xyxy\", \"xywh\", \"cxcywh\"], optional\n        Bounding box format.\n        Supported formats are:&lt;br&gt;\n            - `\"xyxy\"`: boxes are represented via corners,\n                    x1, y1 being top left and x2, y2\n                    being bottom right.&lt;br&gt;\n            - `\"xywh\"`: boxes are represented via corner,\n                    width and height, x1, y2 being top\n                    left, w, h being width and height.\n                    This is the default format; all\n                    input formats will be converted\n                    to this.&lt;br&gt;\n            - `\"cxcywh\"`: boxes are represented via centre,\n                    width and height, cx, cy being\n                    center of box, w, h being width\n                    and height.&lt;br&gt;\n        The default is `\"xywh\"`.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    constructor_model = ConstructorModel.model_validate(\n        {\n            \"iou_thresholds\": iou_thresholds,\n            \"recall_thresholds\": recall_thresholds,\n            \"max_detection_thresholds\": max_detection_thresholds,\n            \"area_ranges\": area_ranges,\n            \"class_metrics\": class_metrics,\n            \"box_format\": box_format,\n            },\n        context={\"default_value\": DEFAULT_COCO, \"default_flag\": _Missing}\n        )\n    self.iou_thresholds: np.ndarray = constructor_model.iou_thresholds\n    self.recall_thresholds: np.ndarray = (\n        constructor_model.recall_thresholds)\n    self.max_detection_thresholds: np.ndarray = (\n        constructor_model.max_detection_thresholds)\n    self.area_ranges: dict[str | None, list[float]] = (\n        constructor_model.area_ranges)\n    self.class_metrics: bool = constructor_model.class_metrics\n    self.box_format: Literal[\"xyxy\", \"xywh\", \"cxcywh\"] = (\n        constructor_model.box_format)\n</code></pre>"},{"location":"api_reference/#src.od_metrics.od_metrics.ODMetrics.compute","title":"<code>compute(y_true, y_pred, extended_summary=False)</code>","text":"<p>Compute metrics.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>list[dict]</code> <p>A list consisting of dictionaries each containing the key-values: each dictionary corresponds to the ground truth of a single image. Parameters that should be provided per dict:</p> <pre><code>boxes : list[list[float]] | np.ndarray\n    List of floats lists or `np.ndarray`; the length of the\n    list/array correspond to the number of boxes and each\n    list/array is 4-float specifying the box coordinates in\n    the format specified in the constructor.\nlabels : list[int] | np.ndarray\n    List of integers or `np.ndarray` specifying the ground\n    truth classes for the boxes: the length corresponds to\n    the number of boxes.\niscrowd : list[bool | Literal[0, 1]] | np.ndarray\n    List of integers or `np.ndarray` specifying crowd regions:\n    the length corresponds to the number of boxes.\n    The values can be `bool` or `0`/`1` indicating whether the\n    bounding box indicates a crowd of objects.\n    Value is optional, and if not provided it will\n    automatically be set to `False`.\narea : list[float] | np.ndarray\n    A list of `float` or `np.ndarray` specifying the area of\n    the objects: the length corresponds to the number of\n    boxes.\n    Value is optional, and if not provided will\n    be automatically calculated based on\n    the bounding box provided.\n    Only affects which samples contribute the specific\n    area range.\n</code></pre> required <code>y_pred</code> <code>list[dict]</code> <p>A list consisting of dictionaries each containing the key-values: each dictionary corresponds to the predictions of a single image. Parameters that should be provided per dict:</p> <pre><code>boxes : list[list[float]] | np.ndarray\n    List of float lists or `np.ndarray`; the length of the\n    list/array correspond to the number of boxes and each\n    list/array is 4-float specifying the box coordinates in\n    the format specified in the constructor.\nscores : list[float] | np.ndarray\n    List of floats or `np.ndarray` specifying the score for\n    the boxes: the length corresponds to the number of boxes.\nlabels : list[int] | np.ndarray\n    List of integers or `np.ndarray` specifying the ground\n    truth classes for the boxes: the length corresponds to\n    the number of boxes.\n</code></pre> required <code>extended_summary</code> <code>bool</code> <p>Option to enable extended summary with additional metrics including <code>IoU</code>, <code>AP</code> (Average Precision), <code>AR</code> (Average Recall) and <code>mean_evaluator</code> (<code>Callable</code>). The output dictionary will contain the following extra key-values:</p> <pre><code>IoU : dict[tuple[int, int], np.ndarray]\n       A dictionary containing the IoU values for every\n       image/class combination e.g. `IoU[(0,0)]`\n       would contain the IoU for image `0` and class `0`.\n       Each value is a `np.ndarray` with shape `(n, m)`\n       where `n` is the number of detections and `m` is\n       the number of ground truth boxes for that image/class\n       combination.\nAP : np.ndarray\n      Average precision: a `np.ndarray` of shape\n      `(T, R, K, A, M)` containing the precision values.\n      Here:\n          - `T` is the number of IoU thresholds\n          - `R` is the number of recall thresholds\n          - `K` is the number of classes\n          - `A` is the number of areas\n          - `M` is the number of max detections per image\nAR : np.ndarray\n     A `np.ndarray` of shape `(T, K, A, M)` containing the\n     averag recall values.\n     Here:\n         - `T` is the number of IoU thresholds\n         - `K` is the number of classes\n         - `A` is the number of areas\n         - `M` is the number of max detections per image\nmean_evaluator : Callable\n    Mean evaluator function.\n    Parameters are:\n        iou_threshold : (float | list[float] | np.ndarray\n                         | None), optional\n            IoU threshold on which calculate the mean.\n            It can be a `float`, a list of floats, `np.ndarray`\n            or `None`; all values must be inlcuded in the\n            constructor argument `iou_thresholds`.\n            If `None`, all input `iou_thresholds` will be used.\n            The default is `None`.\n        area_range_key : (str | list[str] | np.ndarray\n                          | None), optional\n            Area range key on which calculate the mean.\n            It can be a `str`, a list of strings, `np.ndarray`,\n            or `None`; all values must be included in the\n            constructor argument `area_ranges`.\n            If `None`, all input `area_ranges` keys will be\n            used.\n            The default is `None`.\n        max_detection_threshold : (int | list[int] |\n                                   np.ndarray | None), optional\n            Threshold on maxiumum detections per image on\n            which calculate the mean.\n            It can be a `int`, a list of integers, `np.ndarray`\n            or `None`; all values must be inlcuded in the\n            constructor argument `max_detection_thresholds`.\n            If `None`, all input `max_detection_thresholds`\n            will be used.\n            The default is `None`.\n        label_id : (int | list[int] | np.ndarray\n                    | None), optional\n            Label ids on which calculate the mean.\n            If `class_metrics` is `True`, `label_id` must be\n            included in the label ids of the provided `y_true`.\n            If `class_metrics` is `False`, `label_id` must be\n            `-1` (in this case equivalent to `None`).\n            If `None`, all labels will be used.\n            The default is `None`.\n        metrics : (Literal[\"AP\", \"AR\"]\n                   | list[Literal[\"AP\", \"AR\"]] | None),\n                   optional\n            Metrics on which calculate the mean.\n            If `None`, both `\"AP\"` and `\"AR\"` will be used.\n            The default is `None`.\n        include_spec : bool, optional\n            Whether to include mean settings specification.\n            The default is `False`.\n        prefix : str, optional\n            Prefix to add to metrics keys.\n            The default is `m`.\n</code></pre> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float | int | list[int] | dict | ndarray | partial]</code> <p>The format of the output string metric ids is defined as:</p> <p><code>{metric}@[{iou_thresholds} | {area_ranges}           | {max_detection_thresholds}]</code></p> <p>If a field is <code>None</code>, the corresponding string field will be emtpy, e.g., <code>{metric}@[{iou_thresholds} | {area_ranges}]</code> indicate metrics calculated without limit to detections per image, i.e. <code>max_detections_thresholds</code> set to <code>None</code>. Assuming that the parameters passed to the constructor are the default ones (COCO), the output dictionary will contain the following key-values:  <code>mAP@[.5 | all | 100]</code> <code>mAR@[.5 | all | 100]</code> <code>mAP@[.75 | all | 100]</code> <code>mAR@[.75 | all | 100]</code> <code>mAR@[.5:.95 | all | 1]</code> <code>mAR@[.5:.95 | all | 10]</code> <code>mAR@[.5:.95 | all | 100]</code> <code>mAP@[.5:.95 | all | 100]</code> <code>mAP@[.5:.95 | large | 100]</code> <code>mAR@[.5:.95 | large | 100]</code> <code>mAP@[.5:.95 | medium | 100]</code> <code>mAR@[.5:.95 | medium | 100]</code> <code>mAP@[.5:.95 | small | 100]</code> <code>mAR@[.5:.95 | small | 100]</code>  If <code>class_metrics</code> is <code>True</code>, the output dictionary will contain the additional key <code>class_metrics</code>, a dictionary with class as key and value each of the above metrics. If <code>extended_summary</code> is <code>True</code>, the output dictionary will contain the additional keys <code>IoU</code>, <code>AP</code>, <code>AR</code> and <code>mean_evaluator</code>. (See <code>extended_summary</code>)</p> Source code in <code>src/od_metrics/od_metrics.py</code> <pre><code>def compute(\n        self,\n        y_true: list[dict],\n        y_pred: list[dict],\n        extended_summary: bool = False,\n        ) -&gt; dict[str, float | int | list[int]\n                  | dict | np.ndarray | partial]:\n    \"\"\"\n    Compute metrics.\n\n    Parameters\n    ----------\n    y_true : list[dict]\n        A list consisting of dictionaries each containing\n        the key-values: each dictionary corresponds to the ground truth\n        of a single image.\n        Parameters that should be provided per dict:\n\n            boxes : list[list[float]] | np.ndarray\n                List of floats lists or `np.ndarray`; the length of the\n                list/array correspond to the number of boxes and each\n                list/array is 4-float specifying the box coordinates in\n                the format specified in the constructor.\n            labels : list[int] | np.ndarray\n                List of integers or `np.ndarray` specifying the ground\n                truth classes for the boxes: the length corresponds to\n                the number of boxes.\n            iscrowd : list[bool | Literal[0, 1]] | np.ndarray\n                List of integers or `np.ndarray` specifying crowd regions:\n                the length corresponds to the number of boxes.\n                The values can be `bool` or `0`/`1` indicating whether the\n                bounding box indicates a crowd of objects.\n                Value is optional, and if not provided it will\n                automatically be set to `False`.\n            area : list[float] | np.ndarray\n                A list of `float` or `np.ndarray` specifying the area of\n                the objects: the length corresponds to the number of\n                boxes.\n                Value is optional, and if not provided will\n                be automatically calculated based on\n                the bounding box provided.\n                Only affects which samples contribute the specific\n                area range.\n    y_pred : list[dict]\n        A list consisting of dictionaries each containing\n        the key-values: each dictionary corresponds to the predictions\n        of a single image.\n        Parameters that should be provided per dict:\n\n            boxes : list[list[float]] | np.ndarray\n                List of float lists or `np.ndarray`; the length of the\n                list/array correspond to the number of boxes and each\n                list/array is 4-float specifying the box coordinates in\n                the format specified in the constructor.\n            scores : list[float] | np.ndarray\n                List of floats or `np.ndarray` specifying the score for\n                the boxes: the length corresponds to the number of boxes.\n            labels : list[int] | np.ndarray\n                List of integers or `np.ndarray` specifying the ground\n                truth classes for the boxes: the length corresponds to\n                the number of boxes.\n    extended_summary : bool, optional\n        Option to enable extended summary with additional metrics\n        including `IoU`, `AP` (Average Precision), `AR` (Average Recall)\n        and `mean_evaluator` (`Callable`).\n        The output dictionary will contain the following extra key-values:\n\n            IoU : dict[tuple[int, int], np.ndarray]\n                   A dictionary containing the IoU values for every\n                   image/class combination e.g. `IoU[(0,0)]`\n                   would contain the IoU for image `0` and class `0`.\n                   Each value is a `np.ndarray` with shape `(n, m)`\n                   where `n` is the number of detections and `m` is\n                   the number of ground truth boxes for that image/class\n                   combination.\n            AP : np.ndarray\n                  Average precision: a `np.ndarray` of shape\n                  `(T, R, K, A, M)` containing the precision values.\n                  Here:\n                      - `T` is the number of IoU thresholds\n                      - `R` is the number of recall thresholds\n                      - `K` is the number of classes\n                      - `A` is the number of areas\n                      - `M` is the number of max detections per image\n            AR : np.ndarray\n                 A `np.ndarray` of shape `(T, K, A, M)` containing the\n                 averag recall values.\n                 Here:\n                     - `T` is the number of IoU thresholds\n                     - `K` is the number of classes\n                     - `A` is the number of areas\n                     - `M` is the number of max detections per image\n            mean_evaluator : Callable\n                Mean evaluator function.\n                Parameters are:\n                    iou_threshold : (float | list[float] | np.ndarray\n                                     | None), optional\n                        IoU threshold on which calculate the mean.\n                        It can be a `float`, a list of floats, `np.ndarray`\n                        or `None`; all values must be inlcuded in the\n                        constructor argument `iou_thresholds`.\n                        If `None`, all input `iou_thresholds` will be used.\n                        The default is `None`.\n                    area_range_key : (str | list[str] | np.ndarray\n                                      | None), optional\n                        Area range key on which calculate the mean.\n                        It can be a `str`, a list of strings, `np.ndarray`,\n                        or `None`; all values must be included in the\n                        constructor argument `area_ranges`.\n                        If `None`, all input `area_ranges` keys will be\n                        used.\n                        The default is `None`.\n                    max_detection_threshold : (int | list[int] |\n                                               np.ndarray | None), optional\n                        Threshold on maxiumum detections per image on\n                        which calculate the mean.\n                        It can be a `int`, a list of integers, `np.ndarray`\n                        or `None`; all values must be inlcuded in the\n                        constructor argument `max_detection_thresholds`.\n                        If `None`, all input `max_detection_thresholds`\n                        will be used.\n                        The default is `None`.\n                    label_id : (int | list[int] | np.ndarray\n                                | None), optional\n                        Label ids on which calculate the mean.\n                        If `class_metrics` is `True`, `label_id` must be\n                        included in the label ids of the provided `y_true`.\n                        If `class_metrics` is `False`, `label_id` must be\n                        `-1` (in this case equivalent to `None`).\n                        If `None`, all labels will be used.\n                        The default is `None`.\n                    metrics : (Literal[\"AP\", \"AR\"]\n                               | list[Literal[\"AP\", \"AR\"]] | None),\n                               optional\n                        Metrics on which calculate the mean.\n                        If `None`, both `\"AP\"` and `\"AR\"` will be used.\n                        The default is `None`.\n                    include_spec : bool, optional\n                        Whether to include mean settings specification.\n                        The default is `False`.\n                    prefix : str, optional\n                        Prefix to add to metrics keys.\n                        The default is `m`.\n\n    Returns\n    -------\n    dict[str, float | int | list[int] | dict | np.ndarray | partial]\n        The format of the output string metric ids is defined as:\n\n        `{metric}@[{iou_thresholds} | {area_ranges}\n                  | {max_detection_thresholds}]`\n\n        If a field is `None`, the corresponding string field will be emtpy,\n        e.g., `{metric}@[{iou_thresholds} | {area_ranges}]`\n        indicate metrics calculated without limit to detections per image,\n        i.e. `max_detections_thresholds` set to `None`.\n        Assuming that the parameters passed to the constructor\n        are the default ones (COCO), the output dictionary will\n        contain the following key-values: &lt;br&gt;\n            `mAP@[.5 | all | 100]` &lt;br&gt;\n            `mAR@[.5 | all | 100]` &lt;br&gt;\n            `mAP@[.75 | all | 100]` &lt;br&gt;\n            `mAR@[.75 | all | 100]` &lt;br&gt;\n            `mAR@[.5:.95 | all | 1]` &lt;br&gt;\n            `mAR@[.5:.95 | all | 10]` &lt;br&gt;\n            `mAR@[.5:.95 | all | 100]` &lt;br&gt;\n            `mAP@[.5:.95 | all | 100]` &lt;br&gt;\n            `mAP@[.5:.95 | large | 100]` &lt;br&gt;\n            `mAR@[.5:.95 | large | 100]` &lt;br&gt;\n            `mAP@[.5:.95 | medium | 100]` &lt;br&gt;\n            `mAR@[.5:.95 | medium | 100]` &lt;br&gt;\n            `mAP@[.5:.95 | small | 100]` &lt;br&gt;\n            `mAR@[.5:.95 | small | 100]` &lt;br&gt;\n        If `class_metrics` is `True`, the output dictionary will contain\n        the additional key `class_metrics`, a dictionary with class as key\n        and value each of the above metrics.\n        If `extended_summary` is `True`, the output dictionary will contain\n        the additional keys `IoU`, `AP`, `AR` and `mean_evaluator`.\n        (See `extended_summary`)\n    \"\"\"\n    # Image ids\n    images_ids = list(range(len(y_true)))\n\n    # Parse Annotations\n    y_true, y_pred = ComputeModel.model_validate(\n        y_true=y_true,\n        y_pred=y_pred,\n        extended_summary=extended_summary,\n        box_format=self.box_format\n    )\n\n    # Get label_ids from y_true + y_pred\n    label_ids = np.unique([_annotation[\"label_id\"]\n                           for _annotation in y_true + y_pred]).tolist()\n    _label_ids = label_ids if self.class_metrics else [-1]\n\n    # y_true, y_pred --&gt; default_dict (keys: (image_id, label_id))\n    y_true_ddict = defaultdict(\n        list,\n        [(k, list(v)) for k, v in groupby(\n            sorted(y_true, key=lambda x: (x[\"image_id\"], x[\"label_id\"])),\n            key=lambda x: (x[\"image_id\"], x[\"label_id\"]))]\n        )\n    y_pred_ddict = defaultdict(\n        list,\n        [(k, list(v)) for k, v in groupby(\n            sorted(y_pred, key=lambda x: (x[\"image_id\"], x[\"label_id\"])),\n            key=lambda x: (x[\"image_id\"], x[\"label_id\"]))]\n        )\n\n    # Compute IoU\n    ious = {(image_id, label_id): self._compute_iou(\n        y_true=y_true_ddict,\n        y_pred=y_pred_ddict,\n        image_id=image_id,\n        label_id=label_id,\n        label_ids=label_ids,\n        ) for image_id, label_id in product(images_ids, _label_ids)\n        }\n\n    # Evaluate each image\n    images_results = [self._evaluate_image(\n        y_true=y_true_ddict,\n        y_pred=y_pred_ddict,\n        image_id=image_id,\n        label_id=label_id,\n        label_ids=label_ids,\n        area_range=_area_range,\n        ious=ious\n        ) for label_id, _area_range, image_id in\n        product(_label_ids, self.area_ranges.values(), images_ids)\n    ]\n\n    # Aggregate results\n    results = self._aggregate(\n        label_ids=_label_ids,\n        images_ids=images_ids,\n        images_results=images_results,\n        )\n\n    # Mean evaluator\n    mean_evaluator = partial(\n        self._get_mean,\n        results=results,\n        label_ids=_label_ids\n        )\n\n    # Get standard output values (globally)\n    standard_results = self._get_standard(\n        mean_evaluator=mean_evaluator,\n        label_id=None,\n        prefix=\"m\",\n        )\n\n    # Prepare output\n    output: dict[str, float | int | list[int]\n                 | dict | np.ndarray | partial] = {}\n    output |= standard_results\n\n    # Class metrics\n    if self.class_metrics:\n        output |= {\n            \"class_metrics\": {\n                label_id: self._get_standard(\n                    mean_evaluator=mean_evaluator,\n                    label_id=label_id,\n                    prefix=\"\",\n                    ) for label_id in _label_ids\n                    }\n            }\n\n    # Add metadata\n    output |= {\n        \"classes\": label_ids,\n        \"n_images\": len(images_ids)\n        }\n\n    if extended_summary:\n        output |= ({k: v for k, v in results.items() if k in [\"AP\", \"AR\"]}\n                   | {\n                       \"IoU\": ious,\n                       \"mean_evaluator\": mean_evaluator,\n                       }\n                   )\n\n    return output\n</code></pre>"},{"location":"api_reference/#src.od_metrics.od_metrics.iou","title":"<code>iou(y_true, y_pred, iscrowd=None, box_format='xywh')</code>","text":"<p>Calculate IoU between bounding boxes.</p> <p>The standard iou of a ground truth <code>y_true</code> and detected <code>y_pred</code> object is:</p> \\[iou(\\text{y_true}, \\text{y_pred}) =     \\frac{\\text{y_true} \\bigcap \\text{y_pred}}     {\\text{y_true}\\bigcup \\text{y_pred}}\\] Notes <p>For <code>crowd</code> regions, COCO use a modified criteria. If a <code>y_true</code> object is marked as <code>iscrowd</code>, it is permissible for a detected object <code>y_pred</code> to match any subregion of the <code>y_true</code>. Choosing <code>y_true'</code> in the crowd <code>y_true</code> that best matches the <code>y_pred</code> can be done using:</p> \\[\\text{y_true'} = \\text{y_pred} \\bigcap \\text{y_true}\\] <p>Since by definition:</p> \\[ \\text{y_true'} \\bigcup \\text{y_pred} = \\text{y_pred}\\] <p>computing:</p> \\[iou(\\text{y_true}, \\text{y_pred}, \\text{iscrowd}) =     iou(\\text{y_true'}, \\text{y_pred}) =     \\frac{\\text{y_true} \\bigcap \\text{y_pred}}{\\text{y_pred}}\\] <p>For crowd regions in ground truth, this modified criteria for IoU is applied.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>iscrowd</code> and <code>y_true</code> have different length (iscrowd not None).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray | list</code> <p><code>np.ndarray</code> with shape <code>(B1, 4)</code>, <code>B1</code> <code>y_true</code> batch size.</p> required <code>y_pred</code> <code>ndarray | list</code> <p><code>np.ndarray</code> with shape <code>(B2, 4)</code>, <code>B2</code> <code>y_pred</code> batch size.</p> required <code>iscrowd</code> <code>ndarray | list[bool] | list[int] | None</code> <p>Whether <code>y_true</code> are crowd regions. If <code>None</code>, it will be set to <code>False</code> for all <code>y_true</code>. The default is <code>None</code>.</p> <code>None</code> <code>box_format</code> <code>Literal['xyxy', 'xywh', 'cxcywh']</code> <p>Bounding box format. Supported formats are:     - <code>\"xyxy\"</code>: boxes are represented via corners,             x1, y1 being top left and x2, y2             being bottom right.     - <code>\"xywh\"</code>: boxes are represented via corner,             width and height, x1, y2 being top             left, w, h being width and height.             This is the default format; all             input formats will be converted             to this.     - <code>\"cxcywh\"</code>: boxes are represented via centre,             width and height, cx, cy being             center of box, w, h being width             and height. The default is <code>\"xywh\"</code>.</p> <code>'xywh'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>IoU vector of shape <code>(B2, B1)</code>.</p> Source code in <code>src/od_metrics/od_metrics.py</code> <pre><code>def iou(\n        y_true: np.ndarray | list,\n        y_pred: np.ndarray | list,\n        iscrowd: np.ndarray | list[bool] | list[int] | None = None,\n        box_format: Literal[\"xyxy\", \"xywh\", \"cxcywh\"] = \"xywh\",\n        ) -&gt; np.ndarray:\n    \"\"\"\n    Calculate IoU between bounding boxes.\n\n    The standard iou of a ground truth `y_true` and detected\n    `y_pred` object is:\n\n    $$iou(\\\\text{y_true}, \\\\text{y_pred}) =\n        \\\\frac{\\\\text{y_true} \\\\bigcap \\\\text{y_pred}}\n        {\\\\text{y_true}\\\\bigcup \\\\text{y_pred}}$$\n\n    Notes\n    -----\n    For `crowd` regions, COCO use a modified criteria.\n    If a `y_true` object is marked as `iscrowd`, it is permissible\n    for a detected object `y_pred` to match any subregion of the `y_true`.\n    Choosing `y_true'` in the crowd `y_true` that best matches the `y_pred`\n    can be done using:\n\n    $$\\\\text{y_true'} = \\\\text{y_pred} \\\\bigcap \\\\text{y_true}$$\n\n    Since by definition:\n\n    $$ \\\\text{y_true'} \\\\bigcup \\\\text{y_pred} = \\\\text{y_pred}$$\n\n    computing:\n\n    $$iou(\\\\text{y_true}, \\\\text{y_pred}, \\\\text{iscrowd}) =\n        iou(\\\\text{y_true'}, \\\\text{y_pred}) =\n        \\\\frac{\\\\text{y_true} \\\\bigcap \\\\text{y_pred}}{\\\\text{y_pred}}$$\n\n    For crowd regions in ground truth, this modified criteria for IoU\n    is applied.\n\n    Raises\n    ------\n    ValueError\n        If `iscrowd` and `y_true` have different length (iscrowd not None).\n\n    Parameters\n    ----------\n    y_true : np.ndarray | list\n        `np.ndarray` with shape `(B1, 4)`, `B1` `y_true` batch size.\n    y_pred : np.ndarray | list\n        `np.ndarray` with shape `(B2, 4)`, `B2` `y_pred` batch size.\n    iscrowd : np.ndarray | list[bool] | list[int] | None\n        Whether `y_true` are crowd regions.\n        If `None`, it will be set to `False` for all `y_true`.\n        The default is `None`.\n    box_format: Literal[\"xyxy\", \"xywh\", \"cxcywh\"], optional\n        Bounding box format.\n        Supported formats are:&lt;br&gt;\n            - `\"xyxy\"`: boxes are represented via corners,\n                    x1, y1 being top left and x2, y2\n                    being bottom right.&lt;br&gt;\n            - `\"xywh\"`: boxes are represented via corner,\n                    width and height, x1, y2 being top\n                    left, w, h being width and height.\n                    This is the default format; all\n                    input formats will be converted\n                    to this.&lt;br&gt;\n            - `\"cxcywh\"`: boxes are represented via centre,\n                    width and height, cx, cy being\n                    center of box, w, h being width\n                    and height.&lt;br&gt;\n        The default is `\"xywh\"`.\n\n    Returns\n    -------\n    np.ndarray\n        IoU vector of shape `(B2, B1)`.\n    \"\"\"\n    if len(y_pred) == 0 or len(y_true) == 0:\n        return np.array([])\n    # iscrowd\n    if iscrowd is not None:\n        if len(iscrowd) != len(y_true):\n            raise ValueError(\"`iscrowd` and `y_true` should have the same \"\n                             \"length.\")\n    else:\n        iscrowd = [False]*len(y_true)\n    # To np.ndarray and xyxy box format\n    y_true = np.array([to_xyxy(bbox_, box_format) for bbox_ in y_true])\n    y_pred = np.array([to_xyxy(bbox_, box_format) for bbox_ in y_pred])\n\n    # pylint: disable-next=W0632\n    xmin1, ymin1, xmax1, ymax1 = np.hsplit(y_pred, 4)\n    # pylint: disable-next=W0632\n    xmin2, ymin2, xmax2, ymax2 = np.hsplit(y_true, 4)\n\n    # Intersection\n    xmin_i = np.maximum(xmin1.T, xmin2).T\n    ymin_i = np.maximum(ymin1.T, ymin2).T\n    xmax_i = np.minimum(xmax1.T, xmax2).T\n    ymax_i = np.minimum(ymax1.T, ymax2).T\n    inter_area = (np.maximum((xmax_i - xmin_i), 0)\n                  * np.maximum((ymax_i - ymin_i), 0))\n    # Union\n    y_pred_area = (xmax1 - xmin1) * (ymax1 - ymin1)\n    y_true_area = (xmax2 - xmin2) * (ymax2 - ymin2)\n    union_area = y_pred_area + y_true_area.T - inter_area\n    det = np.where(\n        iscrowd,\n        y_pred_area,\n        union_area\n        )\n\n    result = np.divide(inter_area, det, out=np.zeros(inter_area.shape),\n                       where=det != 0)\n    return result\n</code></pre>"},{"location":"iou/","title":"Intersection over Union (IoU)","text":""},{"location":"iou/#what-is-iou","title":"What is IoU?","text":"<p>Intersection Over Union (IoU) is a widely used metric in computer vision that measures the overlap between two bounding boxes. IoU is a crucial metric in object detection because it provides a clear quantification of how well the bounding boxes predicted by the model match the actual objects in the image.</p>"},{"location":"iou/#definition","title":"Definition","text":"<p>IoU (also referred to as the Jaccard Index) between two boxes \\(\\textbf{A}, \\textbf{B} \\in \\mathbb{R}^4\\) is defined as the ratio of their intersection area to their union area. Mathematically:</p> \\[\\text{IoU}(\\textbf{A},\\textbf{B})={{|\\textbf{A}\\cap \\textbf{B}|} \\over {|\\textbf{A}\\cup \\textbf{B}|}}\\]"},{"location":"iou/#how-to-calculate-iou","title":"How to calculate IoU?","text":"<p>Suppose we need to compute the IoU between two boxes \\(\\textbf{A}\\) and \\(\\textbf{B}\\), given in the <code>xyxy</code>(to-left, bottom-right corners) format:</p> \\[\\begin{equation} \\begin{split} &amp; \\textbf{A} = (x^A_1, y^A_1, x^A_2, y^A_2) \\\\ &amp; \\textbf{B} = (x^B_1, y^B_1, x^B_2, y^B_2) \\end{split} \\end{equation}\\] <p>To compute the intersection between \\(\\textbf{A}\\) and \\(\\textbf{B}\\) we define:</p> \\[\\begin{equation} \\begin{split} &amp; x^I_1 = \\max(x^A_1, x^B_1)\\\\ &amp; y^I_1 = \\max(y^A_1, y^B_1)\\\\ &amp; x^I_2 = \\min(x^A_2, x^B_2) \\\\ &amp; y^I_2 = \\min(y^A_2, y^B_2) \\\\ \\end{split} \\end{equation}\\] <p>Then:</p> \\[ |\\textbf{A}\\cap \\textbf{B}| = \\max(x^I_2 - x^I_1, 0) * \\max(y^I_2 - y^I_1) \\] <p>To calculate the union between \\(A\\) and \\(B\\):</p> \\[\\begin{equation} \\begin{split} |\\textbf{A}\\cup \\textbf{B}| &amp; = |\\textbf{A}| + |\\textbf{B}| - |\\textbf{A}\\cap \\textbf{B}|\\\\ &amp; = |x^A_2 - x^A_1| * |y^A_2 - y^A_1| + |x^B_2 - x^B_1| * |y^B_2 - y^B_1| - |\\textbf{A}\\cap \\textbf{B}| \\end{split} \\end{equation}\\]"},{"location":"iou/#example","title":"Example","text":"\\[\\begin{equation} \\begin{split} &amp; \\textbf{A} = (50, 100, 150, 150) \\\\ &amp; \\textbf{B} = (105, 120, 185, 160) \\end{split} \\end{equation}\\] <p>The intersection is calculated as follows:</p> \\[\\begin{equation} \\begin{split} &amp; x^I_1 = \\max(50, 105) = 105\\\\ &amp; y^I_1 = \\max(100, 120) = 120\\\\ &amp; x^I_2 = \\min(150, 185) = 150 \\\\ &amp; y^I_2 = \\min(150, 160) = 150\\\\ \\end{split} \\end{equation}\\] <p>Therefore:</p> \\[ |\\textbf{A}\\cap \\textbf{B}| = \\max(150 - 105, 0) * \\max(150 - 120, 0) = 1350 \\] <p>For the union:</p> \\[ |\\textbf{A}\\cup \\textbf{B}| = |\\textbf{A}| + |\\textbf{B}| - |\\textbf{A}\\cap \\textbf{B}| = 5000 + 3200 - 1350 = 6850 \\] <p>Then:</p> \\[\\text{IoU}(\\textbf{A}, \\textbf{B}) = \\frac{1350}{6850} = 0.19708029\\]"},{"location":"iou/#iscrowd-parameter","title":"<code>iscrowd</code> parameter","text":"<p>OD-Metrics <code>iou</code> function supports <code>iscrowd</code> COCOAPI parameter, which indicates whether a ground truth bounding box \\(\\textbf{y}_{true}\\) represents a crowd of objects. For <code>crowd</code> regions, the IoU metric is computed using a modified criterion: if a \\(\\textbf{y}_{true}\\) object is marked as <code>iscrowd</code>, it is permissible for a detected object \\(\\textbf{y}_{pred}\\) to match any subregion of the \\(\\textbf{y}_{true}\\). Choosing \\(\\textbf{y}'_{true}\\) in the crowd \\(\\textbf{y}_{true}\\) that best matches the \\(\\textbf{y}_{pred}\\) can be done using:</p> \\[\\textbf{y}'_{true} = \\textbf{y}_{pred} \\bigcap \\textbf{y}_{true}\\] <p>Since by definition:</p> \\[ \\textbf{y}'_{true} \\bigcup \\textbf{y}_{pred} = \\textbf{y}_{pred}\\] <p>computing IoU when <code>iscrowd=True</code>, is equivalent to:</p> \\[\\begin{equation} \\begin{split} \\text{IoU}(\\textbf{y}_{true}, \\textbf{y}_{pred}) &amp; = \\text{IoU}(\\textbf{y}'_{true}, \\textbf{y}_{pred})         \\\\ &amp; = \\frac{|\\textbf{y}_{true} \\bigcap \\textbf{y}_{pred}|}{\\textbf{y}_{pred}} \\end{split} \\end{equation}\\] <p>This modified IoU criterion is applied to <code>crowd</code> regions in the ground truth.</p>"},{"location":"iou/#iou-in-od-metrics","title":"IoU in OD-Metrics","text":"<p>OD-Metrics supports IoU metrics with the <code>iou</code> function. The usage is straightfoward. <pre><code>from od_metrics import iou\n\na = [[50, 100, 150, 150]]\nb = [[105, 120, 185, 160]]\n\nresult = iou(a, b, box_format=\"xyxy\")\nprint(result)\n\"\"\"\narray([[0.19708029]])\n\"\"\"\n</code></pre></p> <p>For more examples see Usage or API Reference.</p>"},{"location":"map_mar/","title":"Mean Average Precision (mAP) and Recall (mAR)","text":""},{"location":"map_mar/#what-is-map","title":"What is mAP?","text":"<p>Mean Average Precision (mAP) is the de facto standard accuracy metric in object detection, providing a comprehensive measure of a model's ability to correctly detect and localize objects within an image. This description and methodology follow the approach detailed in R. Padilla et al. <sup>1</sup>.</p>"},{"location":"map_mar/#how-to-calculate-map","title":"How to calculate mAP?","text":"<p>The calculation of mAP involves the Predictions generated by the object detector and the corresponding Ground-truth annotations.</p>"},{"location":"map_mar/#predictions","title":"Predictions","text":"<p>Each prediction of an object detector consists of the following components:</p> <ul> <li>Class \\(\\hat{c} \\in \\mathbb{N}_{\\leq C}^+\\), where \\(C \\in \\mathbb{N}^+\\) is the total number of classes;</li> <li>Bounding Box \\(\\hat{\\textbf{B}} \\in \\mathbb{R}^4\\), specifying the location of the detected object;</li> <li>Confidence Score \\(\\hat{s} \\in [0, 1]\\), indicating the model's confidence in the prediction.</li> </ul> <p>Each detection can be represented as:</p> \\[ \\hat{\\textbf{y}} = ( \\hat{c}, \\hat{\\textbf{B}}, \\hat{s} ) \\]"},{"location":"map_mar/#ground-truth","title":"Ground-truth","text":"<p>Ground-truth annotations provide the reference data against which predictions are evaluated. Each ground-truth annotation consists of the following components:</p> <ul> <li>Class \\(c \\in \\mathbb{N}_{\\leq C}^+\\), the true class label of the object;</li> <li>Bounding Box \\(\\textbf{B} \\in \\mathbb{R}^4\\), the true location of the object.</li> </ul> <p>Each ground-truth object can be represented as:</p> \\[\\textbf{y} = ( c, \\textbf{B} ). \\] <p>To compute mAP, the following sets are used:</p> <ul> <li>a set of \\(G\\) ground-truth objects:</li> </ul> \\[Y = \\{\\textbf{y}_i=(c_i, \\textbf{B}_i) \\}_{i=1,\\ldots,G} \\] <ul> <li>a set of \\(D\\) predictions:</li> </ul> \\[\\hat{Y} = \\{\\hat{\\textbf{y}}_j = (\\hat{c}_j, \\hat{\\textbf{B}}_j, \\hat{s}_j) \\}_{j=1,\\ldots,D} \\]"},{"location":"map_mar/#precision-and-recall","title":"Precision and Recall","text":"<p>To compute Precision (\\(\\text{P}_\\bar{c}\\)) and Recall (\\(\\text{R}_\\bar{c}\\)) for a fixed class \\(\\bar{c} \\in \\mathbb{N}_{\\leq C}^+\\), let's define True Positives (\\(\\text{TP}_\\bar{c}\\)), that are predictions \\(\\hat{\\textbf{y}} = ( \\hat{c}, \\hat{\\textbf{B}}, \\hat{s} )\\) that meet the following criteria:</p> <ul> <li>the predicted class matches the ground-truth class (\\(\\hat{c} = \\bar{c}\\));</li> <li>the IoU between the predicted and ground-truth bounding boxes is greater than or equal to a IoU threshold \\(\\tau_{\\text{IoU}}\\) (\\(\\text{IoU}( \\textbf{B}, \\hat{\\textbf{B}} ) \\geq \\tau_{\\text{IoU}}\\));</li> <li>the confidence score \\(\\hat{s}\\) is greater than or equal to a confidence threshold \\(\\tau_{s} (\\hat{s} \\geq \\tau_{s})\\). </li> </ul> <p>Formally, the definition of \\(\\text{TP}_\\bar{c}\\) (depending on \\(\\tau_{\\text{IoU}}\\) and \\(\\tau_{s}\\)):</p> \\[\\text{TP}_{\\bar{c}}(\\tau_{s}; \\tau_{\\text{IoU}}) = \\{ ( \\hat{c}, \\hat{\\textbf{B}}, \\hat{s} ) \\in \\hat{Y} \\mid \\exists \\; ( \\bar{c}, \\textbf{B} ) \\in Y: \\hat{c} = \\bar{c} \\wedge \\text{IoU}( \\textbf{B}, \\hat{\\textbf{B}} ) \\geq \\tau_{\\text{IoU}} \\wedge \\hat{s} \\geq \\tau_{s} \\}. \\] <p>Thus, we can define Precision (\\(\\text{P}_\\bar{c}\\)) as:</p> \\[ \\text{P}_{\\bar{c}}(\\tau_{s}; \\tau_{\\text{IoU}}) = \\frac{\\text{TP}_{\\bar{c}}(\\tau_{s}; \\tau_{\\text{IoU}})}{\\text{TP}_{\\bar{c}}(\\tau_{s}; \\tau_{\\text{IoU}}) + \\text{FP}_{\\bar{c}}(\\tau_{s}; \\tau_{\\text{IoU}})} = \\frac{|\\text{TP}_{\\bar{c}}(\\tau_{s}; \\tau_{\\text{IoU}})|}{|\\hat{Y}_{\\bar{c}, \\tau_{s}}|}\\] <p>where \\(\\hat{Y}_{\\bar{c}, \\tau_{s}} = \\{\\hat{y} = ( \\hat{c}, \\hat{\\textbf{B}}, \\hat{s} ) \\in \\hat{Y} \\mid \\hat{c} = \\bar{c} \\wedge \\hat{s} \\geq \\tau_{s}\\}\\).</p> <p>Similarly for the Recall (\\(\\text{R}_\\bar{c}\\)):</p> \\[ \\text{R}_{\\bar{c}}(\\tau_{s}; \\tau_{\\text{IoU}}) = \\frac{\\text{TP}_{\\bar{c}}(\\tau_{s}; \\tau_{\\text{IoU}})}{\\text{TP}_{\\bar{c}}(\\tau_{s}; \\tau_{\\text{IoU}}) + \\text{FN}_{\\bar{c}}(\\tau_{s}; \\tau_{\\text{IoU}})} = \\frac{|\\text{TP}_{\\bar{c}}(\\tau_{s}; \\tau_{\\text{IoU}})|}{|Y_{\\bar{c}}|}\\] <p>where \\(Y_{\\bar{c}} = \\{ y = (c, \\textbf{B}) \\in Y \\mid c = \\bar{c} \\}\\).</p>"},{"location":"map_mar/#average-precision","title":"Average Precision","text":"<p>For a specific class \\(\\bar{c}\\) and a fixed IoU threhsold \\(\\bar{\\tau}_{IoU}\\), the Average Precision \\(\\text{AP}_{\\bar{c}}@[\\bar{\\tau}_{IoU}]\\) is a metric based on the area under a \\(\\text{P}_{\\bar{c}}(\\tau_{s}; \\bar{\\tau}_{\\text{IoU}})\\times \\text{R}_{\\bar{c}}(\\tau_{s}; \\bar{\\tau}_{\\text{IoU}})\\) curve:</p> \\[\\text{AP}_{\\bar{c}}@[\\bar{\\tau}_{\\text{IoU}}] = \\int_0^1 \\text{P}_{\\bar{c}}(\\text{R}_{\\bar{c}}; \\bar{\\tau}_{\\text{IoU}}) d\\text{R}_{\\bar{c}} \\] <p>This area is in practice replaced with a finite sum using certain recall values and different interpolation methods. One starts by ordering the \\(K\\) different confidence scores output by the detector, for the specific class \\(\\bar{c}\\):</p> \\[\\{ \\tau_{s_k}, k \\in \\mathbb{N}_{\\leq K}^+ \\mid \\tau_{s_i} &gt; \\tau_{s_j} \\; \\forall i &gt; j\\}\\] <p>Since the \\(\\text{R}_{\\bar{c}}\\) values have a one-to-one, monotonic correspondence with \\(\\tau_{s_k}\\), which has a one-to-one, monotonic, correspondence with the index \\(k\\), then the Precision-Recall curve is not continuous but sampled at the discrete points \\(\\text{R}_{\\bar{c}}(\\tau_{s_k};\\bar{\\tau}_{\\text{IoU}})\\), leading to the set of pairs \\((\\text{P}_{\\bar{c}}(\\tau_{s_k};\\bar{\\tau}_{\\text{IoU}}),\\text{R}_{\\bar{c}}(\\tau_{s_k};\\bar{\\tau}_{\\text{IoU}}))\\) indexed by \\(k\\). Now one defines an ordered set of reference recall values \\(\\text{R}_r\\):</p> \\[\\{ \\text{R}_{r_n}, n \\in \\mathbb{N}_{\\leq N}^+ \\mid \\text{R}_{r_m} &lt; \\text{R}_{r_n} \\; \\forall m &gt; n \\}\\] <p>The Average Precision \\(\\text{AP}_{\\bar{c}}\\) is computed using the two ordered sets \\(\\{ \\tau_{s_k} \\}_{k \\in \\mathbb{N}_{\\leq K}^+}\\) and \\(\\{ \\text{R}_{r_n} \\}_{n \\in \\mathbb{N}_{\\leq N}^+}\\). But before computing \\(\\text{AP}_{\\bar{c}}\\), the Precision-Recall pairs have to be interpolated such that the resulting Precision-Recall curve is monotonic. The resulting interpolated curve is defined by a continuous function \\(\\tilde{\\text{P}}_{\\bar{c}}(x; \\bar{\\tau}_{\\text{IoU}})\\), where \\(x\\) is a real value contained in the interval \\([0, 1]\\), defined as:</p> \\[\\tilde{\\text{P}}_{\\bar{c}}(x; \\bar{\\tau}_{\\text{IoU}}) = \\max_{k \\in \\mathbb{N}_{\\leq K}^+ \\mid \\text{R}_{\\bar{c}}(\\tau_{s_k}, \\bar{\\tau}_{\\text{IoU}}) \\geq x} \\text{P}_{\\bar{c}}(\\tau_{s_k}; \\bar{\\tau}_{\\text{IoU}})\\] <p>The precision value interpolated at recall \\(x\\) corresponds to the maximum precision \\(\\text{P}_{\\bar{c}}(\\tau_{s_k}; \\bar{\\tau}_{\\text{IoU}})\\) whose corresponding recall value is greater than or equal to \\(x\\).</p>"},{"location":"map_mar/#n-point-interpolation","title":"N-Point Interpolation","text":"<p>In the \\(N\\)-point interpolation, the set of reference recall values \\(\\{ \\text{R}_{r_n} \\}_{n \\in \\mathbb{N}_{\\leq N}^+}\\) are equally spaced in the interval \\([0, 1]\\) that is:</p> \\[\\text{R}_{r_n} = \\frac{N - n}{N -1}, \\; \\; n \\in \\mathbb{N}^+_{\\leq N}\\] <p>and:</p> \\[ \\text{AP}_{\\bar{c}}@[\\bar{\\tau}_{\\text{IoU}}] = \\frac{1}{N} \\sum_{n=1}^N  \\tilde{\\text{P}}_{\\bar{c}}(\\frac{N - n}{N -1}; \\bar{\\tau}_{\\text{IoU}}) \\] <p>Popular choices include \\(N=101\\) as in MS-COCO <sup>2</sup> detection competition, and \\(N=11\\), initially adopted by the PASCAL-VOC <sup>3</sup>, which later transitioned to the all-point interpolation method.</p> <p><code>OD-Metrics</code></p> <p>Since <code>OD-Metrics</code> adopts the MS-COCO <sup>2</sup> standard, it uses the \\(N\\)-point interpolation.</p>"},{"location":"map_mar/#all-point-interpolation","title":"All-Point Interpolation","text":"<p>In the so-called all-point interpolation, the set values \\(\\{ \\text{R}_{r_n} \\}_{n \\in \\mathbb{N}_{\\leq N}^+}\\) corresponds exactly to the set of recall values computed considering all \\(K\\) confidence levels \\(\\{ \\tau_{s_k} \\}_{k \\in \\mathbb{N}_{\\leq K}^+}\\):</p> \\[ \\text{AP}_{\\bar{c}}@[\\bar{\\tau}_{\\text{IoU}}] = \\sum_{k=0}^{K} (\\text{R}_\\bar{c}(\\tau_{s_k}; \\bar{\\tau}_{\\text{IoU}}) - \\text{R}_\\bar{c}(\\tau_{s_{k+1}}; \\bar{\\tau}_{\\text{IoU}})) \\tilde{\\text{P}}_{\\bar{c}}(\\text{R}_\\bar{c}(\\tau_{s_k}; \\bar{\\tau}_{\\text{IoU}}); \\bar{\\tau}_{\\text{IoU}}) \\] <p>with \\(\\tau_{s_0} = 0\\), \\(\\text{R}_\\bar{c}(\\tau_{s_0}; \\bar{\\tau}_{\\text{IoU}}) = 1\\), \\(\\tau_{s_{K+1}} = 1\\), \\(\\text{R}_\\bar{c}(\\tau_{s_{K+1}}; \\bar{\\tau}_{\\text{IoU}}) = 0\\).</p>"},{"location":"map_mar/#mean-average-precision","title":"Mean Average Precision","text":"<p>Regardless of the interpolation method, the Average Precision \\(\\text{AP}_{\\bar{c}}@[\\bar{\\tau}_{IoU}]\\) is obtained individually for each class \\(\\bar{c}\\). In large datasets, it is useful to have a unique metric value that is able to represent the accuracy of the detections among all \\(C\\) classes. For such cases, the Mean Average Precision \\(\\text{mAP}@[\\bar{\\tau}_{\\text{IoU}}]\\) is computed, which is simply:</p> \\[\\text{mAP}@[\\bar{\\tau}_{\\text{IoU}}] = \\frac{1}{C}\\sum_{c=1}^C \\text{AP}_{c}@[\\bar{\\tau}_{\\text{IoU}}] \\] <p>In certain competitions, the final metric \\(\\text{mAP}@[T]\\) is computed as the average over a predefined set \\(T\\) of IoU thresholds \\(\\tau_{\\text{IoU}}\\). For instance, in MS-COCO <sup>2</sup>, \\(T\\) is defined as \\(\\{0.5, 0.55, \\ldots, 0.95\\}\\) (increments of 0.05) and it is commonly denoted as \\(\\text{mAP}@[0.5:0.95]\\).</p>"},{"location":"map_mar/#average-recall","title":"Average Recall","text":"<p>Following the definition used in MS-COCO <sup>2</sup>, given a set \\(T\\) of IoU thresholds \\(\\tau_{\\text{IoU}}\\) and a specific class \\(\\bar{c}\\), the Average Recall \\(\\text{AR}_{\\bar{c}}@[T]\\) is defined as:</p> \\[ \\text{AR}_{\\bar{c}}@[T] = \\frac{1}{|T|} \\sum_{\\tau_{\\text{IoU}} \\in T} \\max_{k \\in \\mathbb{N}_{\\leq K}^+} \\text{R}_\\bar{c}(\\tau_{s_k}; \\tau_{\\text{IoU}})\\] <p>The Mean Average Recall \\(\\text{mAR}@[T]\\) is then calculated as the average of \\(\\text{AR}_{\\bar{c}}@[T]\\) across all \\(C\\) classes:</p> \\[ \\text{mAR}@[T] = \\frac{1}{C}\\sum_{c=1}^C \\text{AR}_{c}@[T] \\] <p>In MS-COCO <sup>2</sup>, \\(T = \\{0.5, 0.55, \\ldots, 0.95\\}\\) and the corresponding Mean Average Recall is denoted as \\(\\text{mAR}@[0.5:0.95]\\).</p>"},{"location":"map_mar/#references","title":"References","text":"<ol> <li> <p>Rafael Padilla, Wesley L. Passos, Thadeu L. B. Dias, Sergio L. Netto, and Eduardo A. B. da Silva. A comparative analysis of object detection metrics with a companion open-source toolkit. Electronics, 2021. URL: https://www.mdpi.com/2079-9292/10/3/279, doi:10.3390/electronics10030279.\u00a0\u21a9</p> </li> <li> <p>Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, 740\u2013755. Springer, 2014.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Mark Everingham, SM Ali Eslami, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes challenge: a retrospective. International journal of computer vision, 111:98\u2013136, 2015.\u00a0\u21a9</p> </li> </ol>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#try-live-demo","title":"Try live Demo","text":"<p>Explore live <code>OD-Metrics</code> examples on Binder  or Google Colab  </p>"},{"location":"usage/#simple-example","title":"Simple example.","text":"<p>Consider a scenario with two images, Image 1 and Image 2, and the following annotations and predictions.</p> <p> Image 1 contains:</p> <ul> <li><code>2</code> ground-truth bounding boxes, one for class <code>0</code> and one for class <code>1</code>;</li> <li><code>3</code> predicted bounding boxes with <code>labels</code> <code>[0, 1, 1]</code> and <code>scores</code> <code>[.88, .70, .80]</code>. <pre><code># Image 1\ny_true =\n  {\n   \"boxes\": [[25, 16, 38, 56], [129, 123, 41, 62]],\n   \"labels\": [0, 1]\n   }\ny_pred =\n  {\n   \"boxes\": [[25, 27, 37, 54], [119, 111, 40, 67], [124, 9, 49, 67]],\n   \"labels\": [0, 1, 1],\n   \"scores\": [.88, .70, .80]\n   },\n</code></pre></li> </ul> <p> Image 2 contains:</p> <ul> <li><code>2</code> ground-truth bounding boxes, both for class <code>0</code>;</li> <li><code>3</code> predicted bounding boxes, with <code>labels</code> <code>[0, 1, 0]</code> and <code>scores</code> <code>[.71, .54, .74]</code>. <pre><code># Image 2\ny_true =\n  {\n   \"boxes\": [[123, 11, 43, 55], [38, 132, 59, 45]],\n   \"labels\": [0, 0]\n   }\ny_pred = {\n   \"boxes\": [[64, 111, 64, 58], [26, 140, 60, 47], [19, 18, 43, 35]],\n   \"labels\": [0, 1, 0],\n   \"scores\": [.71, .54, .74]\n   }\n</code></pre> </li> </ul> <p>The mAP (Mean Average Precision) and mAR (Mean Average Recall) for this scenario are computed using <code>OD-Metrics</code> as follows. simple_example<pre><code>from od_metrics import ODMetrics\n\n# Ground truths\ny_true = [\n    { # image 1\n     \"boxes\": [[25, 16, 38, 56], [129, 123, 41, 62]],\n     \"labels\": [0, 1]\n     },\n    { # image 2\n     \"boxes\": [[123, 11, 43, 55], [38, 132, 59, 45]],\n     \"labels\": [0, 0]\n     }\n    ]\n\n# Predictions\ny_pred = [\n    { # image 1\n     \"boxes\": [[25, 27, 37, 54], [119, 111, 40, 67], [124, 9, 49, 67]],\n     \"labels\": [0, 1, 1],\n     \"scores\": [.88, .70, .80]\n     },\n    { # image 2\n     \"boxes\": [[64, 111, 64, 58], [26, 140, 60, 47], [19, 18, 43, 35]],\n     \"labels\": [0, 1, 0],\n     \"scores\": [.71, .54, .74]\n     }\n    ]\n\nmetrics = ODMetrics()\noutput = metrics.compute(y_true, y_pred)\nprint(output)\n\"\"\"\n{\n    \"mAP@[.5 | all | 100]\": 0.16831683168316827,\n    \"mAP@[.5:.95 | all | 100]\": 0.06732673267326732,\n    \"mAP@[.5:.95 | large | 100]\": -1.0,\n    \"mAP@[.5:.95 | medium | 100]\": 0.06732673267326732,\n    \"mAP@[.5:.95 | small | 100]\": -1.0,\n    \"mAP@[.75 | all | 100]\": 0.0,\n    \"mAR@[.5 | all | 100]\": 0.16666666666666666,\n    \"mAR@[.5:.95 | all | 100]\": 0.06666666666666667,\n    \"mAR@[.5:.95 | all | 10]\": 0.06666666666666667,\n    \"mAR@[.5:.95 | all | 1]\": 0.06666666666666667,\n    \"mAR@[.5:.95 | large | 100]\": -1.0,\n    \"mAR@[.5:.95 | medium | 100]\": 0.06666666666666667,\n    \"mAR@[.5:.95 | small | 100]\": -1.0,\n    \"mAR@[.75 | all | 100]\": 0.0,\n    \"class_metrics\": {\n        \"0\": {\n            \"AP@[.5 | all | 100]\": 0.33663366336633654,\n            \"AP@[.5:.95 | all | 100]\": 0.13465346534653463,\n            \"AP@[.5:.95 | large | 100]\": -1.0,\n            \"AP@[.5:.95 | medium | 100]\": 0.13465346534653463,\n            \"AP@[.5:.95 | small | 100]\": -1.0,\n            \"AP@[.75 | all | 100]\": 0.0,\n            \"AR@[.5 | all | 100]\": 0.3333333333333333,\n            \"AR@[.5:.95 | all | 100]\": 0.13333333333333333,\n            \"AR@[.5:.95 | all | 10]\": 0.13333333333333333,\n            \"AR@[.5:.95 | all | 1]\": 0.13333333333333333,\n            \"AR@[.5:.95 | large | 100]\": -1.0,\n            \"AR@[.5:.95 | medium | 100]\": 0.13333333333333333,\n            \"AR@[.5:.95 | small | 100]\": -1.0,\n            \"AR@[.75 | all | 100]\": 0.0\n        },\n        \"1\": {\n            \"AP@[.5 | all | 100]\": 0.0,\n            \"AP@[.5:.95 | all | 100]\": 0.0,\n            \"AP@[.5:.95 | large | 100]\": -1.0,\n            \"AP@[.5:.95 | medium | 100]\": 0.0,\n            \"AP@[.5:.95 | small | 100]\": -1.0,\n            \"AP@[.75 | all | 100]\": 0.0,\n            \"AR@[.5 | all | 100]\": 0.0,\n            \"AR@[.5:.95 | all | 100]\": 0.0,\n            \"AR@[.5:.95 | all | 10]\": 0.0,\n            \"AR@[.5:.95 | all | 1]\": 0.0,\n            \"AR@[.5:.95 | large | 100]\": -1.0,\n            \"AR@[.5:.95 | medium | 100]\": 0.0,\n            \"AR@[.5:.95 | small | 100]\": -1.0,\n            \"AR@[.75 | all | 100]\": 0.0\n        }\n    },\n    \"classes\": [\n        0,\n        1\n    ],\n    \"n_images\": 2\n}\n\"\"\"\n</code></pre></p>"},{"location":"usage/#custom-settings","title":"Custom settings","text":"<p>By default, <code>OD-Metrics</code> follows MS-COCO <sup>1</sup> settings, including <code>iou_thresholds</code>, <code>recall_thresholds</code>, <code>max_detection_thresholds</code>, <code>area_ranges</code>, and <code>class_metrics</code> (see ODMetrics.__init__() method). Custom settings can replace the default configuration. For instance, to set an IoU threshold of <code>0.4</code> and a maximum detection threshold of <code>2</code>:</p> custom_settings_example<pre><code>from od_metrics import ODMetrics\n\n# Ground truths\ny_true = [\n    { # image 1\n     \"boxes\": [[25, 16, 38, 56], [129, 123, 41, 62]],\n     \"labels\": [0, 1]\n     },\n    { # image 2\n     \"boxes\": [[123, 11, 43, 55], [38, 132, 59, 45]],\n     \"labels\": [0, 0]\n     }\n    ]\n\n# Predictions\ny_pred = [\n    { # image 1\n     \"boxes\": [[25, 27, 37, 54], [119, 111, 40, 67], [124, 9, 49, 67]],\n     \"labels\": [0, 1, 1],\n     \"scores\": [.88, .70, .80]\n     },\n    { # image 2\n     \"boxes\": [[64, 111, 64, 58], [26, 140, 60, 47], [19, 18, 43, 35]],\n     \"labels\": [0, 1, 0],\n     \"scores\": [.71, .54, .74]\n     }\n    ]\n\nmetrics = ODMetrics(iou_thresholds=.4, max_detection_thresholds=2)\noutput = metrics.compute(y_true, y_pred)\nprint(output)\n\"\"\"\n{\n    \"mAP@[.4 | all | 2]\": 0.4183168316831683,\n    \"mAP@[.4 | large | 2]\": -1.0,\n    \"mAP@[.4 | medium | 2]\": 0.4183168316831683,\n    \"mAP@[.4 | small | 2]\": -1.0,\n    \"mAR@[.4 | all | 2]\": 0.6666666666666666,\n    \"mAR@[.4 | large | 2]\": -1.0,\n    \"mAR@[.4 | medium | 2]\": 0.6666666666666666,\n    \"mAR@[.4 | small | 2]\": -1.0,\n    \"class_metrics\": {\n        \"0\": {\n            \"AP@[.4 | all | 2]\": 0.33663366336633654,\n            \"AP@[.4 | large | 2]\": -1.0,\n            \"AP@[.4 | medium | 2]\": 0.33663366336633654,\n            \"AP@[.4 | small | 2]\": -1.0,\n            \"AR@[.4 | all | 2]\": 0.3333333333333333,\n            \"AR@[.4 | large | 2]\": -1.0,\n            \"AR@[.4 | medium | 2]\": 0.3333333333333333,\n            \"AR@[.4 | small | 2]\": -1.0\n        },\n        \"1\": {\n            \"AP@[.4 | all | 2]\": 0.5,\n            \"AP@[.4 | large | 2]\": -1.0,\n            \"AP@[.4 | medium | 2]\": 0.5,\n            \"AP@[.4 | small | 2]\": -1.0,\n            \"AR@[.4 | all | 2]\": 1.0,\n            \"AR@[.4 | large | 2]\": -1.0,\n            \"AR@[.4 | medium | 2]\": 1.0,\n            \"AR@[.4 | small | 2]\": -1.0\n        }\n    },\n    \"classes\": [\n        0,\n        1\n    ],\n    \"n_images\": 2\n}\n\"\"\"\n</code></pre>"},{"location":"usage/#class_metrics","title":"<code>class_metrics</code>","text":"<p>If <code>True</code>, evaluation is performed per class: detections are matched to ground truths only if they share the same <code>label_id</code>. If <code>False</code>, evaluation is category-agnostic. When <code>True</code>, the output includes a <code>\"class_metrics\"</code> dictionary with per-class results. This corresponds to <code>useCats</code> in the COCO evaluation protocol. If not specified the default (COCO) is used and corresponds to <code>True</code>. By setting <code>class_metrics=False</code>, the evaluation is category-agnostic. class_metrics_example<pre><code>from od_metrics import ODMetrics\n\n# Ground truths\ny_true = [\n    { # image 1\n     \"boxes\": [[25, 16, 38, 56], [129, 123, 41, 62]],\n     \"labels\": [0, 1]\n     },\n    { # image 2\n     \"boxes\": [[123, 11, 43, 55], [38, 132, 59, 45]],\n     \"labels\": [0, 0]\n     }\n    ]\n\n# Predictions\ny_pred = [\n    { # image 1\n     \"boxes\": [[25, 27, 37, 54], [119, 111, 40, 67], [124, 9, 49, 67]],\n     \"labels\": [0, 1, 1],\n     \"scores\": [.88, .70, .80]\n     },\n    { # image 2\n     \"boxes\": [[64, 111, 64, 58], [26, 140, 60, 47], [19, 18, 43, 35]],\n     \"labels\": [0, 1, 0],\n     \"scores\": [.71, .54, .74]\n     }\n    ]\n\nmetrics = ODMetrics(class_metrics=False)\noutput = metrics.compute(y_true, y_pred)\nprint(output)\n\"\"\"\n{\n    \"mAP@[.5 | all | 100]\": 0.2574257425742574,\n    \"mAP@[.5:.95 | all | 100]\": 0.10297029702970294,\n    \"mAP@[.5:.95 | large | 100]\": -1.0,\n    \"mAP@[.5:.95 | medium | 100]\": 0.10297029702970294,\n    \"mAP@[.5:.95 | small | 100]\": -1.0,\n    \"mAP@[.75 | all | 100]\": 0.0,\n    \"mAR@[.5 | all | 100]\": 0.25,\n    \"mAR@[.5:.95 | all | 100]\": 0.1,\n    \"mAR@[.5:.95 | all | 10]\": 0.1,\n    \"mAR@[.5:.95 | all | 1]\": 0.1,\n    \"mAR@[.5:.95 | large | 100]\": -1.0,\n    \"mAR@[.5:.95 | medium | 100]\": 0.1,\n    \"mAR@[.5:.95 | small | 100]\": -1.0,\n    \"mAR@[.75 | all | 100]\": 0.0,\n    \"classes\": [\n        0,\n        1\n    ],\n    \"n_images\": 2\n}\n\"\"\"\n</code></pre></p>"},{"location":"usage/#extended_summary","title":"<code>extended_summary</code>","text":"<p>The <code>extended_summary</code> option in the ODMetrics.compute() method enables an extended summary with additional metrics such as <code>IoU</code>, <code>AP</code> (Average Precision), <code>AR</code> (Average Recall), and <code>mean_evaluator</code> (a <code>Callable</code>).</p> <p>extended_summary_example<pre><code>from od_metrics import ODMetrics\n\n# Ground truths\ny_true = [\n    { # image 1\n     \"boxes\": [[25, 16, 38, 56], [129, 123, 41, 62]],\n     \"labels\": [0, 1]\n     },\n    { # image 2\n     \"boxes\": [[123, 11, 43, 55], [38, 132, 59, 45]],\n     \"labels\": [0, 0]\n     }\n    ]\n\n# Predictions\ny_pred = [\n    { # image 1\n     \"boxes\": [[25, 27, 37, 54], [119, 111, 40, 67], [124, 9, 49, 67]],\n     \"labels\": [0, 1, 1],\n     \"scores\": [.88, .70, .80]\n     },\n    { # image 2\n     \"boxes\": [[64, 111, 64, 58], [26, 140, 60, 47], [19, 18, 43, 35]],\n     \"labels\": [0, 1, 0],\n     \"scores\": [.71, .54, .74]\n     }\n    ]\n\nmetrics = ODMetrics()\noutput = metrics.compute(y_true, y_pred, extended_summary=True)\nprint(list(output.keys()))\n\"\"\"\n['mAP@[.5 | all | 100]',,\n 'mAP@[.5:.95 | all | 100]',\n 'mAP@[.5:.95 | large | 100]',\n 'mAP@[.5:.95 | medium | 100]',\n 'mAP@[.5:.95 | small | 100]',\n 'mAP@[.75 | all | 100]',\n 'mAR@[.5 | all | 100]',\n 'mAR@[.5:.95 | all | 100]',\n 'mAR@[.5:.95 | all | 10]',\n 'mAR@[.5:.95 | all | 1]',\n 'mAR@[.5:.95 | large | 100]',\n 'mAR@[.5:.95 | medium | 100]',\n 'mAR@[.5:.95 | small | 100]',\n 'mAR@[.75 | all | 100]',\n 'classes',\n 'n_images',\n 'AP',\n 'AR',\n 'IoU',\n 'mean_evaluator']\n\"\"\"\n</code></pre> In particular, <code>mean_evaluator</code> is a <code>Callable</code> that can calculate metrics for any combination of settings, even those not included in default <code>compute</code> output. For example, with standard MS-COCO <sup>1</sup> settings, the metric combination <code>mAP@[.55 | medium | 10]</code> is not included in the default <code>compute</code> output but can be obtained using the <code>mean_evaluator</code>, after calling <code>compute</code>.</p> <p>mean_evaluator_example<pre><code>from od_metrics import ODMetrics\n\n# Ground truths\ny_true = [\n    { # image 1\n     \"boxes\": [[25, 16, 38, 56], [129, 123, 41, 62]],\n     \"labels\": [0, 1]\n     },\n    { # image 2\n     \"boxes\": [[123, 11, 43, 55], [38, 132, 59, 45]],\n     \"labels\": [0, 0]\n     }\n    ]\n\n# Predictions\ny_pred = [\n    { # image 1\n     \"boxes\": [[25, 27, 37, 54], [119, 111, 40, 67], [124, 9, 49, 67]],\n     \"labels\": [0, 1, 1],\n     \"scores\": [.88, .70, .80]\n     },\n    { # image 2\n     \"boxes\": [[64, 111, 64, 58], [26, 140, 60, 47], [19, 18, 43, 35]],\n     \"labels\": [0, 1, 0],\n     \"scores\": [.71, .54, .74]\n     }\n    ]\n\nmetrics = ODMetrics()\noutput = metrics.compute(y_true, y_pred, extended_summary=True)\nmean_evaluator = output[\"mean_evaluator\"]\n_metric = mean_evaluator(\n    iou_threshold=.55,\n    max_detection_threshold=10,\n    area_range_key=\"medium\",\n    metrics=\"AP\"\n    )\nprint(_metric)\n\"\"\"\n{'mAP@[.55 | medium | 10]': 0.16831683168316827}\n\"\"\"\n</code></pre> For a complete list of arguments accepted by the <code>mean_evaluator</code> function, refer to the <code>extended_summary</code> option in the ODMetrics.compute() method.</p>"},{"location":"usage/#iou","title":"<code>IoU</code>","text":"<p>The calculation of mAP and mAR relies on IoU (Intersection over Union). You can use the standalone <code>iou</code> function from <code>OD-Metrics</code>. iou_example<pre><code>from od_metrics import iou\n\ny_true = [[25, 16, 38, 56], [129, 123, 41, 62]]\ny_pred = [[25, 27, 37, 54], [119, 111, 40, 67], [124, 9, 49, 67]]\n\nresult = iou(y_true, y_pred, box_format=\"xywh\")\nprint(result)\n\"\"\"\narray([[0.67655425, 0.        ],\n       [0.        , 0.46192609],\n       [0.        , 0.        ]])\n\"\"\"\n</code></pre> The <code>iou</code> function supports the <code>iscrowd</code> parameter from the COCOAPI. For more details, refer to the iscrowd section.</p>"},{"location":"usage/#references","title":"References","text":"<ol> <li> <p>Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, 740\u2013755. Springer, 2014.\u00a0\u21a9\u21a9</p> </li> </ol>"}]}